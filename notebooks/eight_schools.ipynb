{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eight Schools - An Edward/Stan Comparison\n",
    "\n",
    "In this tutorial we fit the 8 schools model to data made famous by __[Stan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)__,  __[Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/)__ and first appearing in __[Rubin (1981) - (paywalled)](http://journals.sagepub.com/doi/abs/10.3102/10769986006004377)__.  The tutorial will be particularly suited to Stan users who are familiar with hierarchical models that would like to start using Edward.\n",
    "\n",
    "\n",
    "In this example we introduce a slightly modified 8 Schools example and we compare Stan using the No-U-Turn sampler (NUTS) and Edward using Automatic Differentiation Variational Inference (ADVI/KLQP) and Hamiltonian Monte Carlo (HMC).  Both the Stan and Edward code are included in this tutorial and you may choose to install __[pystan](http://pystan.readthedocs.io/en/latest/getting_started.html)__ or __[rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)__ to run the Stan portions of code.  Comparison of Stan and Edward implementations can be a good way to check the algorithm and model is correct.  As we use our own Stan implementation of 8 schools our results will differ slightly with the improper priors used in the Stan __[getting started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)__ page and the Normal/Cauchy priors used in __[Diagnosing Biased Inference with Divergences](http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)__.  \n",
    "\n",
    "We use the non-centered parameterisation as advocated in __[Diagnosing Biased Inference with Divergences](http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)__ - which is highly recommended reading, just note all three models use slightly different priors when comparing results.\n",
    "\n",
    "In the centered (and simplified) form eight schools is a hierarchichal model:\n",
    "\n",
    "$\\mu \\sim \\mathcal{N}(0,10)$\n",
    "\n",
    "$\\log \\tau \\sim \\mathcal{N}(5,1)$\n",
    "\n",
    "$\\theta \\sim \\mathcal{N}(\\mu, (e^{ \\log \\tau})^2)$\n",
    "\n",
    "$y_j \\sim \\mathcal{N}(\\theta, \\sigma^2_j)$\n",
    "\n",
    "where $j \\in [1..8]$ and $\\{ y_j, \\sigma_j \\}$ are observed.\n",
    "\n",
    "It may be easier to think of $\\tau$ as the root variance, which we give a log normal prior to.\n",
    "\n",
    "\n",
    "\n",
    "The Non-centered model is equivalent and more suitable for inference and takes the following form:\n",
    "\n",
    "$\\mu \\sim \\mathcal{N}(0,10)$\n",
    "\n",
    "$\\log \\tau \\sim \\mathcal{N}(5,1)$\n",
    "\n",
    "$\\theta' \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "$y_j \\sim \\mathcal{N}(\\mu + (e^{ \\log \\tau}) \\theta', \\sigma_j^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "J = 8\n",
    "data_y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n",
    "data_sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edward Model Definition\n",
    "\n",
    "Let's now define the same model using Edward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d.rohde/tensorflow3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from edward.models import Normal, Empirical\n",
    "\n",
    "mu = Normal([0.], [10.])\n",
    "logtau = Normal([5.], [1.])\n",
    "theta_prime = Normal(tf.zeros(J), tf.ones(J))\n",
    "sigma = tf.placeholder(tf.float32, J)\n",
    "y = Normal(mu + tf.exp(logtau) * theta_prime, sigma * tf.ones([J]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {y: data_y, sigma: data_sigma}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edward Automatic Differentiation Variational Inference\n",
    "\n",
    "We now use ed.KLqp to perform ADVI.  To do this we define the variational approximation family (as Gaussian) and then optimize the lower bound using Edwards ed.KLqp function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17400/60000 [ 28%] ████████                       ETA: 122s | Loss: 35.726"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('q_logtau'):\n",
    "    q_logtau = Normal(tf.get_variable('loc', [1]),\n",
    "                      tf.nn.softplus(tf.get_variable('scale', [1])))\n",
    "\n",
    "with tf.variable_scope('q_mu'):        \n",
    "    q_mu = Normal(tf.get_variable('loc', [1]),\n",
    "                  tf.nn.softplus(tf.get_variable('scale', [1])))\n",
    "\n",
    "with tf.variable_scope('q_theta_prime'):\n",
    "    q_theta_prime = Normal(tf.get_variable('loc', [J]),\n",
    "                           tf.nn.softplus(tf.get_variable('scale', [J])))\n",
    "\n",
    "\n",
    "inference = ed.KLqp({logtau: q_logtau, mu: q_mu, theta_prime: q_theta_prime},\n",
    "                    data=data)\n",
    "inference.run(n_samples=15, n_iter=60000)\n",
    "\n",
    "\n",
    "klqp_theta_prime_med = np.array(q_theta_prime.mean().eval())\n",
    "klqp_theta_prime_hi =  np.array(q_theta_prime.mean().eval() \\\n",
    "                                + 1.96 * q_theta_prime.stddev().eval())\n",
    "klqp_theta_prime_low = np.array(q_theta_prime.mean().eval() \\\n",
    "                                - 1.96 * q_theta_prime.stddev().eval())\n",
    "\n",
    "S = 400000\n",
    "klqp_theta = np.tile(q_mu.sample(S).eval(), [1, 8]) \\\n",
    "                     + np.tile(np.exp(q_logtau.sample(S).eval()), \n",
    "                               [1, 8]) * q_theta_prime.sample(S).eval()\n",
    "klqp_theta_low = np.array([np.percentile(klqp_theta[:,ii],2.5) for ii in range(8)])\n",
    "klqp_theta_med = np.array([np.percentile(klqp_theta[:,ii],50) for ii in range(8)])\n",
    "klqp_theta_hi = np.array([np.percentile(klqp_theta[:,ii],97.5) for ii in range(8)])\n",
    "\n",
    "print(\"====    ed.KLqp inference ====\")\n",
    "print(\"E[mu] = %f\" % (q_mu.mean().eval()))\n",
    "print(\"E[logtau] = %f\" % (q_logtau.mean().eval()))\n",
    "print(\"E[theta_prime]=\")\n",
    "print((q_theta_prime.mean().eval()))\n",
    "print(\"====  end ed.KLqp inference ====\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edward HMC inference\n",
    "\n",
    "We now fit the same Edward model using HMC which is similar to Stan's NUTS algorithm but has more algorithm parameters to tune (although here we just use Edward's defaults).\n",
    "\n",
    "This is similar to ADVI except that the approximating family is Edward's Empirical type and the sampling is performed using Edwards ed.HMC function.  Note: unlike Stan's self tuning NUTS algoirthm HMC does need to be tuned for a particular problem.  Here we find the defaults work acceptably, but this cannot be assumed in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 400000\n",
    "burn = S // 2\n",
    "hq_logtau = Empirical(tf.Variable(tf.zeros([S, 1])))\n",
    "hq_mu = Empirical(tf.Variable(tf.zeros([S, 1])))\n",
    "hq_theta_prime = Empirical(tf.Variable(tf.zeros([S, J])))\n",
    "\n",
    "inference = ed.HMC({logtau: hq_logtau, mu: hq_mu,\n",
    "                    theta_prime: hq_theta_prime}, data=data)\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_theta_prime_low = np.array([np.percentile(\n",
    "    hq_theta_prime.params.eval()[burn:, ii], 2.5) for ii in range(8)])\n",
    "hmc_theta_prime_hi = np.array([np.percentile(\n",
    "    hq_theta_prime.params.eval()[burn:, ii], 97.5) for ii in range(8)])\n",
    "hmc_theta_prime_med = np.array([np.percentile(\n",
    "    hq_theta_prime.params.eval()[burn:, ii], 50) for ii in range(8)])\n",
    "\n",
    "hmc_theta = np.tile(hq_mu.params.eval()[burn:], [1, 8]) + \\\n",
    "                    np.tile(np.exp(hq_logtau.params.eval()[burn:]), \n",
    "                              [1, 8]) * hq_theta_prime.params.eval()[burn:]\n",
    "hmc_theta_low = np.array([np.percentile(hmc_theta[:,ii],2.5) for ii in range(8)])\n",
    "hmc_theta_med = np.array([np.percentile(hmc_theta[:,ii],50) for ii in range(8)])\n",
    "hmc_theta_hi = np.array([np.percentile(hmc_theta[:,ii],97.5) for ii in range(8)])\n",
    "\n",
    "print(\"====    ed.HMC inference ====\")\n",
    "print(\"E[mu] = %f\" % (hq_mu.params.eval()[burn:].mean()))\n",
    "print(\"E[logtau] = %f\" % (hq_logtau.params.eval()[burn:].mean()))\n",
    "print(\"E[theta_prime]=\")\n",
    "print(hq_theta_prime.params.eval()[burn:, ].mean(0))\n",
    "print(\"====  end ed.HMC inference ====\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stan Model Definition\n",
    "\n",
    "The Stan implementation of this non-centered model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_schools_stan = \"\"\"\n",
    "data {\n",
    "  int<lower=0> J;\n",
    "  real y[J];\n",
    "  real<lower=0> sigma[J];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real mu;\n",
    "  real logtau;\n",
    "  real theta_prime[J];\n",
    "}\n",
    "\n",
    "model {\n",
    "  mu ~ normal(0, 10);\n",
    "  logtau ~ normal(5, 1);\n",
    "  theta_prime ~ normal(0, 1);\n",
    "  for (j in 1:J) {\n",
    "      y[j] ~ normal(mu + exp(logtau) * theta_prime[j], sigma[j]);\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stan code quite closely follows the model definition.  Note that putting the Stan code in a string like this is convenient for this tutorial, but usually you should instead put the code in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stan NUTS inference \n",
    "\n",
    "This allows us to run the Stan code (make sure pystan is installed for this step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pystan\n",
    "standata = dict(J=J, y=data_y, sigma=data_sigma)\n",
    "\n",
    "fit = pystan.stan(model_code=eight_schools_stan, \n",
    "                  data=standata, iter=100000)\n",
    "\n",
    "s = fit.extract()\n",
    "\n",
    "stan_theta_prime_low = np.array([np.percentile(s['theta_prime'][:, ii], 2.5)\n",
    "                                for ii in range(8)])\n",
    "stan_theta_prime_hi = np.array([np.percentile(s['theta_prime'][:, ii], 97.5)\n",
    "                                for ii in range(8)])\n",
    "stan_theta_prime_med = np.array([np.percentile(s['theta_prime'][:, ii], 50)\n",
    "                                for ii in range(8)])\n",
    "\n",
    "stan_theta = np.tile(s['mu'], [8, 1]) \\\n",
    "    + np.tile(np.exp(s['logtau']), [8, 1]) * s['theta_prime'].T\n",
    "\n",
    "stan_theta_low = np.array([np.percentile(stan_theta[ii, ], 2.5) for ii in range(8)])\n",
    "stan_theta_hi = np.array([np.percentile(stan_theta[ii, ], 97.5) for ii in range(8)])\n",
    "stan_theta_med = np.array([np.percentile(stan_theta[ii, ], 50) for ii in range(8)])\n",
    "\n",
    "print(\"====  Stan/NUTS inference ====\")\n",
    "print(\"E[mu] = %f\" % (s['mu'].mean()))\n",
    "print(\"E[logtau] = %f\" % (s['logtau'].mean()))\n",
    "print(\"E[theta_prime]=\")\n",
    "print(s['theta_prime'].mean(0))\n",
    "print(\"====  end Stan/NUTS inference ====\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the approximate posteriors for $\\theta'$\n",
    "\n",
    "\n",
    "We can graphically show the the three methods have very similar $95\\%$ intervals for $\\theta'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
    "\n",
    "ax.errorbar(np.array(range(8)), hmc_theta_prime_med,\n",
    "            yerr=[hmc_theta_prime_med - hmc_theta_prime_low,\n",
    "                  hmc_theta_prime_hi - hmc_theta_prime_med], fmt='o')\n",
    "ax.errorbar(np.array(range(8)) + 0.1, klqp_theta_prime_med,\n",
    "            yerr=[klqp_theta_prime_med - klqp_theta_prime_low,\n",
    "                  klqp_theta_prime_hi - klqp_theta_prime_med], fmt='o')\n",
    "ax.errorbar(np.array(range(8)) + 0.2, stan_theta_prime_med,\n",
    "            yerr=[stan_theta_prime_med - stan_theta_prime_low,\n",
    "                  stan_theta_prime_hi - stan_theta_prime_med], fmt='o')\n",
    "ax.legend(('Edward/HMC', 'Edward/KLQP', 'Stan/HMC'))\n",
    "plt.ylabel('$\\\\theta\\'$')\n",
    "plt.xlabel('School')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the approximate posteriors for $\\theta$ and $y$\n",
    "\n",
    "We can graphically show the the three methods have very similar $95\\%$ intervals for $\\theta$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "ax.scatter(np.array(range(8)), hmc_theta_med)\n",
    "ax.scatter(np.array(range(8)) + 0.1, klqp_theta_med)\n",
    "ax.scatter(np.array(range(8)) + 0.2, stan_theta_med)\n",
    "ax.scatter(np.array(range(8)) + 0.1, data_y)\n",
    "\n",
    "\n",
    "ax.errorbar(np.array(range(8)), hmc_theta_med,\n",
    "            yerr=[hmc_theta_med - hmc_theta_low, hmc_theta_hi - hmc_theta_med], fmt='none')\n",
    "\n",
    "\n",
    "ax.errorbar(np.array(range(8)) + 0.1, klqp_theta_med,\n",
    "            yerr=[klqp_theta_med - klqp_theta_low, klqp_theta_hi - klqp_theta_med], fmt='none')\n",
    "\n",
    "ax.errorbar(np.array(range(8)) + 0.2, stan_theta_med,\n",
    "            yerr=[stan_theta_med - stan_theta_low, stan_theta_hi - stan_theta_med], fmt='none')\n",
    "\n",
    "\n",
    "ax.legend(('Edward/HMC', 'Edward/KLQP', 'Stan/HMC', 'Observed y'))\n",
    "\n",
    "plt.xlabel('School')\n",
    "plt.ylabel('$\\\\theta$')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix R code for running through rstan\n",
    "\n",
    "library(rstan)\n",
    "\n",
    "rstan_options(auto_write = TRUE)\n",
    "\n",
    "options(mc.cores = parallel::detectCores())\n",
    "\n",
    "schools_dat <- list(J = 8,\n",
    "                    y = c(28,  8, -3,  7, -1,  1, 18, 12),\n",
    "                    sigma = c(15, 10, 16, 11,  9, 11, 10, 18))\n",
    "\n",
    "\n",
    "fit <- stan(file = 'eight_schools.stan', data = schools_dat,iter = 100000, chains = 4)\n",
    "\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
