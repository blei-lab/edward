{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Schools\n",
    "\n",
    "In this tutorial we fit the 8 schools model to data made famous by __[Stan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)__,  __[Bayesian Data Analysis](http://www.stat.columbia.edu/~gelman/book/)__ and first appearing in __[Rubin (1981) - paywalled](http://journals.sagepub.com/doi/abs/10.3102/10769986006004377)__.  The tutorial will be particularly suited to Stan users who are familiar with hierachichal models that would like to start using Edward.\n",
    "\n",
    "\n",
    "In this example we introduce a slightly modified 8 Schools example and we compare Stan using the No-u-turn sampler (NUTS) and Edward using Automatic Differential Variational Bayes (ADVI/KLQP) and Hamiltonian Monte Carlo (HMC).  Both the Stan and Edward code are included in this tutorial and you may choose to install __[pystan](http://pystan.readthedocs.io/en/latest/getting_started.html)__ or __[rstan](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)__ to run the Stan portions of code.  Comparison of Stan and Edward implementations can be a good way to check the algorithm and model is correct.  As we use our own Stan implementation of 8 schools our results will differ slightly with the improper priors used in the Stan __[getting started](https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started)__ and the Normal/Cauchy priors used in __[Diagnosing Biased Inference with Divergences](http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)__.  \n",
    "\n",
    "We use the non-centred parameterisation as advocated in __[Diagnosing Biased Inference with Divergences](http://mc-stan.org/users/documentation/case-studies/divergences_and_bias.html)__ - which is highly recommended reading, just note all three priors are slightly different and so are the results.\n",
    "\n",
    "In the centred (and simplified) form eight schools is a hierarchichal model:\n",
    "\n",
    "$\\mu \\sim \\mathcal{N}(0,10)$\n",
    "\n",
    "$\\log \\tau \\sim \\mathcal{N}(5,1)$\n",
    "\n",
    "$\\theta \\sim \\mathcal{N}(\\mu, (e^{ \\log \\tau})^2)$\n",
    "\n",
    "$y_j \\sim \\mathcal{N}(\\theta, \\sigma^2_j)$\n",
    "\n",
    "where $j \\in [1..8]$ and $\\{ y_j, \\sigma_j \\}$\n",
    "\n",
    "It may be easier to think of $\\tau$ as the root variance, which we give a log normal prior to.\n",
    "\n",
    "\n",
    "\n",
    "The Non-centred model is equivelent and more suitable for inference and takes the following form:\n",
    "\n",
    "$\\mu \\sim \\mathcal{N}(0,10)$\n",
    "\n",
    "$\\log \\tau \\sim \\mathcal{N}(5,1)$\n",
    "\n",
    "$\\theta' \\sim \\mathcal{N}(0,1)$\n",
    "\n",
    "$y_j \\sim \\mathcal{N}(\\mu + (e^{ \\log \\tau}) \\theta', \\sigma_j^2)$\n",
    "\n",
    "The Stan implementation of this non-centred model is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_schools_stan = \"\"\"\n",
    "data {\n",
    "  int<lower=0> J;\n",
    "  real y[J];\n",
    "  real<lower=0> sigma[J];\n",
    "}\n",
    "\n",
    "parameters {\n",
    "  real mu;\n",
    "  real logtau;\n",
    "  real theta_tilde[J];\n",
    "}\n",
    "\n",
    "model {\n",
    "  mu ~ normal(0, 10);\n",
    "  logtau ~ normal(5, 1);\n",
    "  theta_tilde ~ normal(0, 1);\n",
    "  for (j in 1:J) {\n",
    "      y[j] ~ normal(mu + exp(logtau) * theta_tilde[j], sigma[j]);\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stan code quite closely follows the model definition.  Note that putting the Stan code in a string like this is convenient for this tutorial, but usually you should instead put the code in a separate file.\n",
    "\n",
    "We will also define the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "J = 8\n",
    "data_y =     np.array([28,  8, -3,  7, -1,  1, 18, 12])\n",
    "data_sigma = np.array([15, 10, 16, 11,  9, 11, 10, 18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to run the Stan code (make sure pystan is installed for this step - or use rstan if you are familiar with the R interface to Stan, see eight_schools.R and eight_schools.stan in examples/eight_schools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_9d66c37004ddc3041422a88e70f9afbc NOW.\n"
     ]
    }
   ],
   "source": [
    "standata = dict(J=J, y=data_y, sigma=data_sigma)\n",
    "\n",
    "import pystan\n",
    "fit = pystan.stan(model_code=eight_schools_stan, data=standata, iter=100000)\n",
    "print(fit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inferred parameter theta_tilda or $\\theta'$ is a transform, we might prefer to convert this back to the more intepretable theta by posthoc processing of the samples or by including a generated quantities block in the Stan code.  In this case the required postprocessing is quite simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=fit.extract()\n",
    "theta = np.tile(s['mu'],[8,1]) + np.tile(np.exp(s['logtau']),[8,1]) * s['theta_tilde'].T\n",
    "\n",
    "theta_low = np.array([np.percentile(theta[ii,], 2.5) for ii in range(8)])\n",
    "theta_hi = np.array([np.percentile(theta[ii,], 97.5) for ii in range(8)])\n",
    "theta_med = np.array([np.percentile(theta[ii,], 50) for ii in range(8)])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "ax.scatter(range(8), theta_med)\n",
    "ax.scatter(range(8), data_y)\n",
    "ax.errorbar(range(8), theta_med, yerr=[theta_med-theta_low, theta_hi-theta_med], fmt='none')\n",
    "plt.xlabel('School')\n",
    "plt.ylabel('$\\\\theta$')\n",
    "plt.legend(('estimate','observed'))\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pooling for this model is determined by the prior on the root variance parameter $\\tau$.  By varying the prior on $\\tau$ the model can estimate $\\theta_j$ as being close to $y_j$ or all values of $\\theta_j$ being both similar to each other and close to $\\bar{y}$.  Here we observe a compromise between these two extreme positions, the estimate $\\theta_j$ is between to $\\bar{y}$ and $y_j$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=1, sharex=True)\n",
    "plt.plot([-20,40],[-20,40],'k')\n",
    "ax.scatter(data_y, theta_med)\n",
    "ax.errorbar(data_y, theta_med, yerr=[theta_med-theta_low, theta_hi-theta_med],xerr=data_sigma, fmt='none')\n",
    "plt.xlabel('$y$')\n",
    "plt.ylabel('$\\\\theta$')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot shows how the observed $y_j$ corresponds to it's estimated value $\\theta_j$, without pooling $\\theta_j$ would follow $y_j$ (i.e. the estimate would fall on the black line), with full pooling $\\theta_j$ would equal $\\bar{y}$ we again see we have a compromise here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=fit.extract()\n",
    "stan_theta_tilde_low = np.array([np.percentile(s['theta_tilde'][:,ii],2.5) for ii in range(8)])\n",
    "stan_theta_tilde_hi = np.array([np.percentile(s['theta_tilde'][:,ii],97.5) for ii in range(8)])\n",
    "stan_theta_tilde_med = np.array([np.percentile(s['theta_tilde'][:,ii],50) for ii in range(8)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the same model using Edward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "from edward.models import Normal, Empirical\n",
    "\n",
    "mu = Normal(tf.zeros([1]), tf.fill([1], 10.))\n",
    "logtau = Normal(tf.fill([1], 5.),tf.fill([1], 1.))\n",
    "\n",
    "theta_tilde = Normal(tf.zeros([J]), tf.ones([J]))\n",
    "sigma = tf.placeholder(tf.float32, J)\n",
    "y = Normal(mu + tf.exp(logtau) * theta_tilde, sigma * tf.ones([J]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly we define the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thedata = {y: data_y, sigma: data_sigma}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use ed.KLqp to perform ADVI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_logtau = Normal(tf.Variable(tf.random_normal([1])),\n",
    "                  tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n",
    "q_mu = Normal(tf.Variable(tf.random_normal([1])),\n",
    "              tf.nn.softplus(tf.Variable(tf.random_normal([1]))))\n",
    "q_theta_tilde = Normal(tf.Variable(tf.random_normal([J])),\n",
    "                       tf.nn.softplus(tf.Variable(tf.random_normal([J]))))\n",
    "par2q = {logtau: q_logtau, mu: q_mu, theta_tilde: q_theta_tilde}\n",
    "inference = ed.KLqp(par2q, data=thedata)\n",
    "inference.run(n_samples=15, n_iter=60000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "klqp_theta_tilde_med = np.array(q_theta_tilde.mean().eval())\n",
    "klqp_theta_tilde_hi =  np.array(q_theta_tilde.mean().eval() + 2 * q_theta_tilde.stddev().eval())\n",
    "klqp_theta_tilde_low = np.array(q_theta_tilde.mean().eval() - 2 * q_theta_tilde.stddev().eval())\n",
    "\n",
    "\n",
    "\n",
    "print(\"====    ed.KLqp inference ====\")\n",
    "print(\"E[mu] = %f\" % (q_mu.mean().eval()))\n",
    "print(\"E[logtau] = %f\" % (q_logtau.mean().eval()))\n",
    "print(\"E[theta_tilde]=\")\n",
    "print((q_theta_tilde.mean().eval()))\n",
    "print(\"====  end ed.KLqp inference ====\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now fit the same Edward model using HMC which is similar to Stan's NUTS algorithm but has more algorithm parameters to tune (although here we just use Edward's defaults)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 400000\n",
    "burn = S // 2\n",
    "hq_logtau = Empirical(tf.Variable(tf.zeros([S, 1])))\n",
    "hq_mu = Empirical(tf.Variable(tf.zeros([S, 1])))\n",
    "hq_theta_tilde = Empirical(tf.Variable(tf.zeros([S, J])))\n",
    "\n",
    "par2hq = {logtau: hq_logtau, mu: hq_mu, theta_tilde: hq_theta_tilde}\n",
    "inference = ed.HMC(par2hq, data=thedata)\n",
    "inference.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmc_theta_tilde_low = np.array([np.percentile(hq_theta_tilde.params.eval()[burn:, ii],2.5) for ii in range(8)])\n",
    "hmc_theta_tilde_hi = np.array([np.percentile(hq_theta_tilde.params.eval()[burn:, ii],97.5) for ii in range(8)])\n",
    "hmc_theta_tilde_med= np.array([np.percentile(hq_theta_tilde.params.eval()[burn:, ii],50) for ii in range(8)])\n",
    "\n",
    "\n",
    "\n",
    "print(\"====    ed.HMC inference ====\")\n",
    "print(\"E[mu] = %f\" % (hq_mu.params.eval()[burn:].mean()))\n",
    "print(\"E[logtau] = %f\" % (hq_logtau.params.eval()[burn:].mean()))\n",
    "print(\"E[theta_tilde]=\")\n",
    "print(hq_theta_tilde.params.eval()[burn:, ].mean(0))\n",
    "print(\"====  end ed.HMC inference ====\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can graphically show the the three methods are agreement for $\\theta'$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, sharex=True)\n",
    "\n",
    "ax.errorbar(np.array(range(8)), hmc_theta_tilde_med, yerr=[hmc_theta_tilde_med-hmc_theta_tilde_low, hmc_theta_tilde_hi-hmc_theta_tilde_med], fmt='o')\n",
    "ax.errorbar(np.array(range(8)) + 0.1, klqp_theta_tilde_med, yerr=[klqp_theta_tilde_med-klqp_theta_tilde_low, klqp_theta_tilde_hi-klqp_theta_tilde_med], fmt='o')\n",
    "ax.errorbar(np.array(range(8)) + 0.2, stan_theta_tilde_med, yerr=[stan_theta_tilde_med-stan_theta_tilde_low, stan_theta_tilde_hi-hmc_theta_tilde_med], fmt='o')\n",
    "ax.legend(('Edward/HMC','Edward/KLQP','Stan/HMC'))\n",
    "plt.ylabel('$\\\\theta\\'$')\n",
    "plt.xlabel('School')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
