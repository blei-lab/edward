{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automated Transformations\n",
    "\n",
    "Automated transformations provide convenient handling of constrained\n",
    "continuous variables during inference by transforming them to an\n",
    "unconstrained space. Automated transformations are crucial for\n",
    "expanding the scope of algorithm classes such as gradient-based Monte\n",
    "Carlo and variational inference with reparameterization gradients.\n",
    "\n",
    "A webpage version of this tutorial is available at\n",
    "http://edwardlib.org/tutorials/automated-transformations.\n",
    "\n",
    "## The Transform Primitive\n",
    "\n",
    "Automated transformations in Edward are enabled through the key\n",
    "primitive\n",
    "[`ed.transform`](http://edwardlib.org/api/ed/transform).\n",
    "It takes as input a (possibly constrained) continuous random variable\n",
    "$\\mathbf{x}$, defaults to a choice of transformation $T$, and returns a\n",
    "[`TransformedDistribution`](http://edwardlib.org/api/ed/models/TransformedDistribution)\n",
    "$\\mathbf{y}=T(\\mathbf{x})$ with unconstrained support.\n",
    "An optional argument allows you to manually specify the transformation.\n",
    "\n",
    "The returned random variable $\\mathbf{y}$'s density is the original\n",
    "random variable $\\mathbf{x}$'s density adjusted by the determinant of\n",
    "the Jacobian of the inverse transformation (Casella & Berger, 2002),\n",
    "\n",
    "$$p(\\mathbf{y}) = p(\\mathbf{x})~|\\mathrm{det}~J_{T^{-1}}(\\mathbf{y}) |.$$\n",
    "\n",
    "Intuitively, the Jacobian describes how a transformation warps unit\n",
    "volumes across spaces. This matters for transformations of random\n",
    "variables, since probability density functions must always integrate\n",
    "to one.\n",
    "\n",
    "## Automated Transformations in Inference\n",
    "\n",
    "To use automated transformations during inference, set the flag\n",
    "argument `auto_transform=True` in `inference.initialize`\n",
    "(or the all-encompassing method `inference.run`):\n",
    "\n",
    "```python\n",
    "inference.initialize(auto_transform=True)\n",
    "```\n",
    "\n",
    "By default, the flag is already set to `True`.\n",
    "With this flag, any key-value pair passed into inference's\n",
    "`latent_vars` with unequal support is transformed to the\n",
    "unconstrained space; no transformation is applied if already\n",
    "unconstrained. The algorithm is then run under\n",
    "`inference.latent_vars`, which explicitly stores the\n",
    "transformed latent variables and forgets the constrained ones.\n",
    "\n",
    "We illustrate automated transformations in a few inference examples.\n",
    "Imagine that the target distribution is a Gamma distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from edward.models import Gamma\n",
    "\n",
    "x = Gamma(1.0, 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example is only used for illustration, but note this context of\n",
    "inference with latent variables of non-negative support occur\n",
    "frequently: for example, this appears when applying topic models with a deep exponential\n",
    "family where we might use a normal variational\n",
    "approximation to implicitly approximate latent variables with Gamma\n",
    "priors (in\n",
    "[`examples/deep_exponential_family.py`](https://github.com/blei-lab/edward/blob/master/examples/deep_exponential_family.py),\n",
    "we explicitly define a non-negative variational approximation).\n",
    "\n",
    "__Variational inference.__\n",
    "Consider a Normal variational approximation\n",
    "and use the algorithm [`ed.KLqp`](http://edwardlib.org/api/ed/KLqp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 3s | Loss: -0.078\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Normal\n",
    "\n",
    "qx = Normal(loc=tf.Variable(tf.random_normal([])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([]))))\n",
    "\n",
    "inference = ed.KLqp({x: qx})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gamma and Normal distribution have unequal support, so inference\n",
    "transforms both to the unconstrained space; normal is already\n",
    "unconstrained so only Gamma is transformed. `ed.KLqp` then\n",
    "optimizes with\n",
    "[reparameterization gradients](http://edwardlib.org/api/klqp).\n",
    "This means the normal distribution's parameters are optimized to match\n",
    "the transformed (unconstrained) Gamma distribution.\n",
    "\n",
    "Oftentimes we'd like the approximation on the original (constrained)\n",
    "space. This was never needed for inference, so we must explicitly\n",
    "build it by first obtaining the target distribution's transformation\n",
    "and then inverting it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.distributions import bijectors\n",
    "\n",
    "x_unconstrained = inference.transformations[x]  # transformed prior\n",
    "x_transform = x_unconstrained.bijector  # transformed prior's transformation\n",
    "qx_constrained = ed.transform(qx, bijectors.Invert(x_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of transformed distributions is given by\n",
    "`inference.transformations`, which is a dictionary with keys\n",
    "given by any constrained latent variables and values given by their\n",
    "transformed distribution. We then use the\n",
    "[`bijectors`](https://www.tensorflow.org/versions/master/api_docs/python/tf/distributions/bijectors)\n",
    "module in `tf.distributions` in order to handle invertible\n",
    "transformations.\n",
    "`qx_unconstrained` is a random variable distributed\n",
    "according to a inverse-transformed (constrained) normal distribution.\n",
    "For example, if the automated transformation from non-negative to\n",
    "reals is $\\log$, then the constrained approximation is a LogNormal\n",
    "distribution.\n",
    "\n",
    "__Gradient-based Monte Carlo.__\n",
    "Consider an Empirical approximation with 1000 samples\n",
    "and use the algorithm [`ed.HMC`](http://edwardlib.org/api/ed/HMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 2s | Acceptance Rate: 0.965\n"
     ]
    }
   ],
   "source": [
    "from edward.models import Normal\n",
    "\n",
    "qx = Empirical(params=tf.Variable(tf.random_normal([1000])))\n",
    "\n",
    "inference = ed.HMC({x: qx})\n",
    "inference.run(step_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gamma and Empirical have unequal support so Gamma is transformed to\n",
    "the unconstrained space; by implementation, discrete delta\n",
    "distributions such as Empirical and PointMass are not transformed.\n",
    "`ed.HMC` then simulates \n",
    "Hamiltonian dynamics and writes the\n",
    "unconstrained samples to the empirical distribution.\n",
    "\n",
    "In order to obtain the approximation on the original (constrained)\n",
    "support, we again take the inverse of the target distribution's\n",
    "transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.distributions import bijectors\n",
    "\n",
    "x_unconstrained = inference.transformations[x]  # transformed prior\n",
    "x_transform = x_unconstrained.bijector  # transformed prior's transformation\n",
    "qx_constrained = Empirical(params=x_transform.inverse(qx.params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike variational inference, we don't use `ed.transform` to\n",
    "obtain the constrained approximation, as it only applies to continuous\n",
    "distributions. Instead, we define a new Empirical distribution whose\n",
    "parameters (samples) are given by transforming all samples stored in\n",
    "the unconstrained approximation.\n",
    "\n",
    "## Acknowledgements & Remarks\n",
    "\n",
    "Automated transformations have largely been popularized by Stan,\n",
    "particularly for Hamiltonian Monte Carlo (Carpenter et al., 2016).\n",
    "This design is inspired by Stan's. However, a key distinction is Edward\n",
    "provides users flexibility to wield transformations and manipulate\n",
    "results in either the original (constrained) or inferred\n",
    "(unconstrained) space.\n",
    "\n",
    "Automated transformations are also core to the algorithm automatic\n",
    "differentiation variational inference (Kucukelbir et al., 2017),\n",
    "which allows it to select a default variational family of normal\n",
    "distributions. However, note the automated transformation from\n",
    "non-negative to reals in Edward is not $\\log$, which is used in Stan;\n",
    "rather, Edward uses $\\textrm{softplus}$ which is more numerically\n",
    "stable (see also Kucukelbir et al. (2017, Fig. 9).\n",
    "\n",
    "Finally, note that not all inference algorithms use or even need\n",
    "automated transformations.\n",
    "[`ed.Gibbs`](http://edwardlib.org/api/ed/Gibbs), moment\n",
    "matching with EP using Edward's conjugacy, and\n",
    "[`ed.KLqp`](http://edwardlib.org/api/ed/KLqp)\n",
    "with\n",
    "score function gradients all perform inference on the original latent\n",
    "variable space.\n",
    "Point estimation such as [`ed.MAP`](http://edwardlib.org/api/ed/MAP) also\n",
    "use the original latent variable space and only requires a\n",
    "constrained transformation on unconstrained free parameters.\n",
    "Model parameter estimation such as\n",
    "[`ed.GANInference`](http://edwardlib.org/api/ed/GANInference) do not even\n",
    "perform inference over latent variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
