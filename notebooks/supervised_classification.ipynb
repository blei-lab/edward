{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning (Classification)\n",
    "\n",
    "In supervised learning, the task is to infer hidden structure from\n",
    "labeled data, comprised of training examples $\\{(x_n, y_n)\\}$.\n",
    "Classification means the output $y$ takes discrete values.\n",
    "\n",
    "We demonstrate with an example in Edward. A webpage version is available at\n",
    "http://edwardlib.org/tutorials/supervised-classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Bernoulli, MultivariateNormalTriL, Normal\n",
    "from edward.util import rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Use the\n",
    "[crabs data set](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/crabs.html),\n",
    "which consists of morphological measurements on a crab species. We\n",
    "are interested in predicting whether a given crab has the color form\n",
    "blue or orange."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points: 100\n",
      "Number of features: 5\n"
     ]
    }
   ],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "data = np.loadtxt('data/crabs_train.txt', delimiter=',')\n",
    "data[data[:, 0] == -1, 0] = 0  # replace -1 label with 0 label\n",
    "\n",
    "N = data.shape[0]  # number of data points\n",
    "D = data.shape[1] - 1  # number of features\n",
    "\n",
    "X_train = data[:, 1:]\n",
    "y_train = data[:, 0]\n",
    "\n",
    "print(\"Number of data points: {}\".format(N))\n",
    "print(\"Number of features: {}\".format(D))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "A Gaussian process is a powerful object for modeling nonlinear\n",
    "relationships between pairs of random variables. It defines a distribution over\n",
    "(possibly nonlinear) functions, which can be applied for representing\n",
    "our uncertainty around the true functional relationship.\n",
    "Here we define a Gaussian process model for classification\n",
    "(Rasumussen & Williams, 2006).\n",
    "\n",
    "Formally, a distribution over functions $f:\\mathbb{R}^D\\to\\mathbb{R}$ can be specified\n",
    "by a Gaussian process\n",
    "$$\n",
    "\\begin{align*}\n",
    "  p(f)\n",
    "  &=\n",
    "  \\mathcal{GP}(f\\mid \\mathbf{0}, k(\\mathbf{x}, \\mathbf{x}^\\prime)),\n",
    "\\end{align*}\n",
    "$$\n",
    "whose mean function is the zero function, and whose covariance\n",
    "function is some kernel which describes dependence between\n",
    "any set of inputs to the function.\n",
    "\n",
    "Given a set of input-output pairs\n",
    "$\\{\\mathbf{x}_n\\in\\mathbb{R}^D,y_n\\in\\mathbb{R}\\}$,\n",
    "the likelihood can be written as a multivariate normal\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{y})\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{y} \\mid \\mathbf{0}, \\mathbf{K})\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{K}$ is a covariance matrix given by evaluating\n",
    "$k(\\mathbf{x}_n, \\mathbf{x}_m)$ for each pair of inputs in the data\n",
    "set.\n",
    "\n",
    "The above applies directly for regression where $\\mathbb{y}$ is a\n",
    "real-valued response, but not for (binary) classification, where $\\mathbb{y}$\n",
    "is a label in $\\{0,1\\}$. To deal with classification, we interpret the\n",
    "response as latent variables which is squashed into $[0,1]$. We then\n",
    "draw from a Bernoulli to determine the label, with probability given\n",
    "by the squashed value.\n",
    "\n",
    "Define the likelihood of an observation $(\\mathbf{x}_n, y_n)$ as\n",
    "\n",
    "\\begin{align*}\n",
    "  p(y_n \\mid \\mathbf{z}, x_n)\n",
    "  &=\n",
    "  \\text{Bernoulli}(y_n \\mid \\text{logit}^{-1}(\\mathbf{x}_n^\\top \\mathbf{z})).\n",
    "\\end{align*}\n",
    "\n",
    "Define the prior to be a multivariate normal\n",
    "\n",
    "\\begin{align*}\n",
    "  p(\\mathbf{z})\n",
    "  &=\n",
    "  \\text{Normal}(\\mathbf{z} \\mid \\mathbf{0}, \\mathbf{K}),\n",
    "\\end{align*}\n",
    "\n",
    "with covariance matrix given as previously stated.\n",
    "\n",
    "Let's build the model in Edward. We use a radial basis function (RBF)\n",
    "kernel, also known as the squared exponential or exponentiated\n",
    "quadratic. It returns the kernel matrix evaluated over all pairs of\n",
    "data points; we then Cholesky decompose the matrix to parameterize the\n",
    "multivariate normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "f = MultivariateNormalTriL(loc=tf.zeros(N), scale_tril=tf.cholesky(rbf(X)))\n",
    "y = Bernoulli(logits=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we define a placeholder `X`. During inference, we pass in\n",
    "the value for this placeholder according to data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Perform variational inference.\n",
    "Define the variational model to be a fully factorized normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf = Normal(loc=tf.Variable(tf.random_normal([N])),\n",
    "            scale=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run variational inference for `500` iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "5000/5000 [100%] ██████████████████████████████ Elapsed: 6s | Loss: 82.755\n"
     ]
    }
   ],
   "source": [
    "inference = ed.KLqp({f: qf}, data={X: X_train, y: y_train})\n",
    "inference.run(n_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case\n",
    "`KLqp` defaults to minimizing the\n",
    "$\\text{KL}(q\\|p)$ divergence measure using the reparameterization\n",
    "gradient.\n",
    "For more details on inference, see the [$\\text{KL}(q\\|p)$ tutorial](/tutorials/klqp).\n",
    "(This example happens to be slow because evaluating and inverting full\n",
    "covariances in Gaussian processes happens to be slow.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renyi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.inferences.renyi_divergence import Renyi_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Renyi_divergence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d23734059cd6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRenyi_divergence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mqf\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRMSPropOptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m inference.initialize(optimizer=optimizer,\n\u001b[1;32m      4\u001b[0m                      \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Renyi_divergence' is not defined"
     ]
    }
   ],
   "source": [
    "inference = Renyi_divergence({f: qf}, data={X: X_train, y: y_train})\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n",
    "inference.initialize(optimizer=optimizer,\n",
    "                     n_samples=32,\n",
    "                     batch_size=32,\n",
    "                     alpha=0.2,\n",
    "                     backward_pass='full')\n",
    "inference.run(n_iter=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AB: todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from edward.inferences.ab_divergence import AB_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AB_divergence' object has no attribute 'kl_scaling'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-80c46932b258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                      \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                      \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                      beta=0.6)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/edward/inferences/ab_divergence.pyc\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, n_samples, batch_size, alpha, beta, *args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAB_divergence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/edward/inferences/variational_inference.pyc\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, optimizer, var_list, use_prettytensor, global_step, *args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_loss_and_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/edward/inferences/ab_divergence.pyc\u001b[0m in \u001b[0;36mbuild_loss_and_gradients\u001b[0;34m(self, var_list)\u001b[0m\n\u001b[1;32m     75\u001b[0m         is_analytic_kl = all([isinstance(z, Normal) and isinstance(qz, Normal)\n\u001b[1;32m     76\u001b[0m                               for z, qz in six.iteritems(self.latent_vars)])\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_analytic_kl\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkl_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m             \u001b[0;31m# raise NotImplementedError(\"non analytic KL not implemented yet\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"kl_scaling must be None when using non-analytic KL term\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'AB_divergence' object has no attribute 'kl_scaling'"
     ]
    }
   ],
   "source": [
    "inference = AB_divergence({f: qf}, data={X: X_train, y: y_train})\n",
    "optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n",
    "inference.initialize(optimizer=optimizer,\n",
    "                     n_samples=32,\n",
    "                     batch_size=32,\n",
    "                     alpha=0.1,\n",
    "                     beta=0.6)\n",
    "inference.run(n_iter=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
