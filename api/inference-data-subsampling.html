<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Data Subsampling</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/api/">API</a>
    <hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
    <a class="button u-full-width" href="/api/reference">Reference</a>
    <a class="button u-full-width" href="/api/ed/criticisms">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models">ed.models</a>
<a class="button u-full-width" href="/api/ed/util">ed.util</a>
<a class="button u-full-width" href="/api/observations">observations</a>

    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">
<h1 id="api-and-documentation">API and Documentation</h1>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4 button-primary" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="data-subsampling">Data Subsampling</h3>
<p>Running algorithms which require the full data set for each update can be expensive when the data is large. In order to scale inferences, we can do <em>data subsampling</em>, i.e., update inference using only a subsample of data at a time.</p>
<p>(Note that only certain algorithms support data subsampling such as <code>MAP</code>, <code>KLqp</code>, and <code>SGLD</code>. Also, below we illustrate data subsampling for hierarchical models; for models with only global variables such as Bayesian neural networks, a simpler solution exists: see the <a href="/tutorials/batch-training">batch training tutorial</a>.)</p>
<h3 id="subgraphs">Subgraphs</h3>
<p><img src="/images/hierarchical_model_subgraph.png" alt="image" width="250" /><br />
<span><em>Data subsampling with a hierarchical model. We define a subgraph of the full model, forming a plate of size <span class="math inline">\(M\)</span> rather than <span class="math inline">\(N\)</span>.</em></span></p>
<p>In the subgraph setting, we do data subsampling while working with a subgraph of the full model. This setting is necessary when the data and model do not fit in memory. It is scalable in that both the algorithm’s computational complexity (per iteration) and memory complexity are independent of the data set size.</p>
<p>As illustration, consider a hierarchical model, <span class="math display">\[p(\mathbf{x}, \mathbf{z}, \beta)
= p(\beta) \prod_{n=1}^N p(z_n \mid \beta) p(x_n \mid z_n, \beta),\]</span> where there are latent variables <span class="math inline">\(z_n\)</span> for each data point <span class="math inline">\(x_n\)</span> (local variables) and latent variables <span class="math inline">\(\beta\)</span> which are shared across data points (global variables).</p>
<p>To avoid memory issues, we work on only a subgraph of the model, <span class="math display">\[p(\mathbf{x}, \mathbf{z}, \beta)
= p(\beta) \prod_{m=1}^M p(z_m \mid \beta) p(x_m \mid z_m, \beta).\]</span> More concretely, we might define a mixture of Gaussians over <span class="math inline">\(D\)</span>-dimensional data <span class="math inline">\(\{x_n\}\in\mathbb{R}^{N\times D}\)</span>. There are <span class="math inline">\(K\)</span> latent cluster means <span class="math inline">\(\{\beta_k\}\in\mathbb{R}^{K\times D}\)</span> and a membership assignment <span class="math inline">\(z_n\in\{0,\ldots,K-1\}\)</span> for each data point <span class="math inline">\(x_n\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, Normal

N = 10000000  # data set size
M = 128  # minibatch size
D = 2  # data dimensionality
K = 5  # number of clusters

beta = Normal(loc=tf.zeros([K, D]), scale=tf.ones([K, D]))
z = Categorical(logits=tf.zeros([M, K]))
x = Normal(loc=tf.gather(beta, z), scale=tf.ones([M, D]))</code></pre>
<p>For inference, the variational model follows the same factorization as the posterior, <span class="math display">\[q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{n=1}^N q(z_n \mid \beta; \gamma_n),\]</span> parameterized by <span class="math inline">\(\{\lambda, \{\gamma_n\}\}\)</span>. Again, we work on only a subgraph of the model, <span class="math display">\[q(\mathbf{z}, \beta) =
q(\beta; \lambda) \prod_{m=1}^M q(z_m \mid \beta; \gamma_m).\]</span> parameterized by <span class="math inline">\(\{\lambda, \{\gamma_m\}\}\)</span>. Importantly, only <span class="math inline">\(M\)</span> parameters are stored in memory for <span class="math inline">\(\{\gamma_m\}\)</span> rather than <span class="math inline">\(N\)</span>.</p>
<pre class="python" language="Python"><code>qbeta = Normal(loc=tf.Variable(tf.zeros([K, D])),
               scale=tf.nn.softplus(tf.Variable(tf.zeros[K, D])))
qz_variables = tf.Variable(tf.zeros([M, K]))
qz = Categorical(logits=qz_variables)</code></pre>
<p>For illustration, we perform inference with <code>KLqp</code>, a variational method that minimizes the divergence measure <span class="math inline">\(\text{KL}(q\| p)\)</span>.</p>
<p>We instantiate two algorithms: a global inference over <span class="math inline">\(\beta\)</span> given the subset of <span class="math inline">\(\mathbf{z}\)</span> and a local inference over the subset of <span class="math inline">\(\mathbf{z}\)</span> given <span class="math inline">\(\beta\)</span>. We also pass in a TensorFlow placeholder <code>x_ph</code> for the data, so we can change the data at each step. (Alternatively, <a href="/api/data">batch tensors</a> can be used.)</p>
<pre class="python" language="Python"><code>x_ph = tf.placeholder(tf.float32, [M])
inference_global = ed.KLqp({beta: qbeta}, data={x: x_ph, z: qz})
inference_local = ed.KLqp({z: qz}, data={x: x_ph, beta: qbeta})</code></pre>
<p>We initialize the algorithms with the <code>scale</code> argument, so that computation on <code>z</code> and <code>x</code> will be scaled appropriately. This enables unbiased estimates for stochastic gradients.</p>
<pre class="python" language="Python"><code>inference_global.initialize(scale={x: float(N) / M, z: float(N) / M})
inference_local.initialize(scale={x: float(N) / M, z: float(N) / M})</code></pre>
<p>Conceptually, the scale argument represents scaling for each random variable’s plate, as if we had seen that random variable <span class="math inline">\(N/M\)</span> as many times.</p>
<p>We now run inference, assuming there is a <code>next_batch</code> function which provides the next batch of data.</p>
<pre class="python" language="Python"><code>qz_init = tf.initialize_variables([qz_variables])
for _ in range(1000):
  x_batch = next_batch(size=M)
  for _ in range(10):  # make local inferences
    inference_local.update(feed_dict={x_ph: x_batch})

  # update global parameters
  inference_global.update(feed_dict={x_ph: x_batch})
  # reinitialize the local factors
  qz_init.run()</code></pre>
<p>After each iteration, we also reinitialize the parameters for <span class="math inline">\(q(\mathbf{z}\mid\beta)\)</span>; this is because we do inference on a new set of local variational factors for each batch.</p>
<p>This demo readily applies to other inference algorithms such as <code>SGLD</code> (stochastic gradient Langevin dynamics): simply replace <code>qbeta</code> and <code>qz</code> with <code>Empirical</code> random variables; then call <code>ed.SGLD</code> instead of <code>ed.KLqp</code>.</p>
<h3 id="advanced-settings">Advanced settings</h3>
<p>If the parameters fit in memory, one can avoid having to reinitialize local parameters or read/write from disk. To do this, define the full set of parameters and index them into the local posterior factors.</p>
<pre class="python" language="Python"><code>qz_variables = tf.Variable(tf.zeros([N, K]))
idx_ph = tf.placeholder(tf.int32, [M])
qz = Categorical(logits=tf.gather(qz_variables, idx_ph))</code></pre>
<p>We define an index placeholder <code>idx_ph</code>. It will be fed index values at runtime to determine which parameters correspond to a given data subsample. As an example, see the script for <a href="https://github.com/blei-lab/edward/blob/master/examples/probabilistic_pca_subsampling.py">probabilistic principal components analysis</a> with stochastic variational inference.</p>
<p>An alternative approach to reduce memory complexity is to use an inference network <span class="citation">(Dayan, Hinton, Neal, &amp; Zemel, 1995)</span>, also known as amortized inference <span class="citation">(Stuhlmüller, Taylor, &amp; Goodman, 2013)</span>. This can be applied using a global parameterization of <span class="math inline">\(q(\mathbf{z}, \beta)\)</span>. For more details, see the <a href="/tutorials/inference-networks">inference networks tutorial</a>.</p>
<p>In streaming data, or online inference, the size of the data <span class="math inline">\(N\)</span> may be unknown, or conceptually the size of the data may be infinite and at any time in which we query parameters from the online algorithm, the outputted parameters are from having processed as many data points up to that time. The approach of Bayesian filtering <span class="citation">(Broderick, Boyd, Wibisono, Wilson, &amp; Jordan, 2013; Doucet, Godsill, &amp; Andrieu, 2000)</span> can be applied in Edward using recursive posterior inferences; the approach of population posteriors <span class="citation">(McInerney, Ranganath, &amp; Blei, 2015)</span> is readily applicable from the subgraph setting.</p>
<p>In other settings, working on a subgraph of the model does not apply, such as in time series models when we want to preserve dependencies across time steps in our variational model. Approaches in the literature can be applied in Edward <span class="citation">(Binder, Murphy, &amp; Russell, 1997; Foti, Xu, Laird, &amp; Fox, 2014; Johnson &amp; Willsky, 2014)</span>.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-binder1997space">
<p>Binder, J., Murphy, K., &amp; Russell, S. (1997). Space-efficient inference in dynamic probabilistic networks. In <em>International joint conference on artificial intelligence</em>.</p>
</div>
<div id="ref-broderick2013streaming">
<p>Broderick, T., Boyd, N., Wibisono, A., Wilson, A. C., &amp; Jordan, M. I. (2013). Streaming variational Bayes. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-dayan1995helmholtz">
<p>Dayan, P., Hinton, G. E., Neal, R. M., &amp; Zemel, R. S. (1995). The Helmholtz machine. <em>Neural Computation</em>, <em>7</em>(5), 889–904.</p>
</div>
<div id="ref-doucet2000on">
<p>Doucet, A., Godsill, S., &amp; Andrieu, C. (2000). On sequential Monte Carlo sampling methods for Bayesian filtering. <em>Statistics and Computing</em>, <em>10</em>(3), 197–208.</p>
</div>
<div id="ref-foti2014stochastic">
<p>Foti, N., Xu, J., Laird, D., &amp; Fox, E. (2014). Stochastic variational inference for hidden Markov models. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-johnson2014stochastic">
<p>Johnson, M., &amp; Willsky, A. S. (2014). Stochastic variational inference for Bayesian time series models. In <em>International conference on machine learning</em>.</p>
</div>
<div id="ref-mcinerney2015population">
<p>McInerney, J., Ranganath, R., &amp; Blei, D. M. (2015). The Population Posterior and Bayesian Inference on Streams. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-stuhlmuller2013learning">
<p>Stuhlmüller, A., Taylor, J., &amp; Goodman, N. (2013). Learning stochastic inverses. In <em>Neural information processing systems</em>.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
