<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Composing Inferences</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/api/">API</a>
    <hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
    <a class="button u-full-width" href="/api/reference">Reference</a>
    <a class="button u-full-width" href="/api/ed/criticisms">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models">ed.models</a>
<a class="button u-full-width" href="/api/ed/util">ed.util</a>
<a class="button u-full-width" href="/api/observations">observations</a>

    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">
<h1 id="api-and-documentation">API and Documentation</h1>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4" href="/api/inference-classes">Classes</a>
<a class="button4 button-primary" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="composing-inferences">Composing Inferences</h3>
<p>Core to Edward’s design is compositionality. Compositionality enables fine control of inference, where we can write inference as a collection of separate inference programs.</p>
<p>We outline how to write popular classes of compositional inferences using Edward: hybrid algorithms and message passing algorithms. We use the running example of a mixture model with latent mixture assignments <code>z</code>, latent cluster means <code>beta</code>, and observations <code>x</code>.</p>
<h3 id="hybrid-algorithms">Hybrid algorithms</h3>
<p>Hybrid algorithms leverage different inferences for each latent variable in the posterior. As an example, we demonstrate variational EM, with an approximate E-step over local variables and an M-step over global variables. We alternate with one update of each <span class="citation">(Neal &amp; Hinton, 1993)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, PointMass

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference_e = ed.VariationalInference({z: qz}, data={x: x_data, beta: qbeta})
inference_m = ed.MAP({beta: qbeta}, data={x: x_data, z: qz})
...
for _ in range(10000):
  inference_e.update()
  inference_m.update()</code></pre>
<p>In <code>data</code>, we include bindings of prior latent variables (<code>z</code> or <code>beta</code>) to posterior latent variables (<code>qz</code> or <code>qbeta</code>). This performs conditional inference, where only a subset of the posterior is inferred while the rest are fixed using other inferences.</p>
<p>This extends to many algorithms: for example, exact EM for exponential families; contrastive divergence <span class="citation">(Hinton, 2002)</span>; pseudo-marginal and ABC methods <span class="citation">(Andrieu &amp; Roberts, 2009)</span>; Gibbs sampling within variational inference <span class="citation">(Wang &amp; Blei, 2012)</span>; Laplace variational inference <span class="citation">(Wang &amp; Blei, 2013)</span>; and structured variational auto-encoders <span class="citation">(Johnson, Duvenaud, Wiltschko, Datta, &amp; Adams, 2016)</span>.</p>
<h3 id="message-passing-algorithms">Message passing algorithms</h3>
<p>Message passing algorithms operate on the posterior distribution using a collection of local inferences <span class="citation">(Koller &amp; Friedman, 2009)</span>. As an example, we demonstrate expectation propagation. We split a mixture model to be over two random variables <code>x1</code> and <code>x2</code> along with their latent mixture assignments <code>z1</code> and <code>z2</code>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, Normal

N1 = 1000  # number of data points in first data set
N2 = 2000  # number of data points in second data set
D = 2  # data dimension
K = 5  # number of clusters

# MODEL
beta = Normal(loc=tf.zeros([K, D]), scale=tf.ones([K, D]))
z1 = Categorical(logits=tf.zeros([N1, K]))
z2 = Categorical(logits=tf.zeros([N2, K]))
x1 = Normal(loc=tf.gather(beta, z1), scale=tf.ones([N1, D]))
x2 = Normal(loc=tf.gather(beta, z2), scale=tf.ones([N2, D]))

# INFERENCE
qbeta = Normal(loc=tf.Variable(tf.zeros([K, D])),
               scale=tf.nn.softplus(tf.Variable(tf.zeros([K, D]))))
qz1 = Categorical(logits=tf.Variable(tf.zeros[N1, K]))
qz2 = Categorical(logits=tf.Variable(tf.zeros[N2, K]))

inference_z1 = ed.KLpq({beta: qbeta, z1: qz1}, {x1: x1_train})
inference_z2 = ed.KLpq({beta: qbeta, z2: qz2}, {x2: x2_train})
...
for _ in range(10000):
  inference_z1.update()
  inference_z2.update()</code></pre>
<p>We alternate updates for each local inference, where the global posterior factor <span class="math inline">\(q(\beta)\)</span> is shared across both inferences <span class="citation">(Gelman et al., 2017)</span>.</p>
<p>With TensorFlow’s distributed training, compositionality enables <em>distributed</em> message passing over a cluster with many workers. The computation can be further sped up with the use of GPUs via data and model parallelism.</p>
<p>This extends to many algorithms: for example, classical message passing, which performs exact local inferences; Gibbs sampling, which draws samples from conditionally conjugate inferences <span class="citation">(S. Geman &amp; Geman, 1984)</span>; expectation propagation, which locally minimizes <span class="math inline">\(\text{KL}(p || q)\)</span> over exponential families <span class="citation">(Minka, 2001)</span>; integrated nested Laplace approximation, which performs local Laplace approximations <span class="citation">(Rue, Martino, &amp; Chopin, 2009)</span>; and all the instantiations of EP-like algorithms in <span class="citation">Gelman et al. (2017)</span>.</p>
<p>In the above, we perform local inferences split over individual random variables. At the moment, Edward does not support local inferences within a random variable itself. We cannot do local inferences when representing the random variable for all data points and their cluster membership as <code>x</code> and <code>z</code> rather than <code>x1</code>, <code>x2</code>, <code>z1</code>, and <code>z2</code>.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-andrieu2009pseudo">
<p>Andrieu, C., &amp; Roberts, G. O. (2009). The pseudo-marginal approach for efficient Monte Carlo computations. <em>The Annals of Statistics</em>, 697–725.</p>
</div>
<div id="ref-gelman2017expectation">
<p>Gelman, A., Vehtari, A., Jylänki, P., Sivula, T., Tran, D., Sahai, S., … Robert, C. (2017). Expectation propagation as a way of life: A framework for Bayesian inference on partitioned data. <em>ArXiv Preprint ArXiv:1412.4869v2</em>.</p>
</div>
<div id="ref-geman1984stochastic">
<p>Geman, S., &amp; Geman, D. (1984). Stochastic relaxation, gibbs distributions, and the bayesian restoration of images. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, (6), 721–741.</p>
</div>
<div id="ref-hinton2002training">
<p>Hinton, G. E. (2002). Training products of experts by minimizing contrastive divergence. <em>Neural Computation</em>, <em>14</em>(8), 1771–1800.</p>
</div>
<div id="ref-johnson2016composing">
<p>Johnson, M. J., Duvenaud, D., Wiltschko, A. B., Datta, S. R., &amp; Adams, R. P. (2016). Composing graphical models with neural networks for structured representations and fast inference. <em>ArXiv Preprint ArXiv:1603.06277</em>.</p>
</div>
<div id="ref-koller2009probabilistic">
<p>Koller, D., &amp; Friedman, N. (2009). <em>Probabilistic graphical models: Principles and techniques</em>. MIT press.</p>
</div>
<div id="ref-minka2001expectation">
<p>Minka, T. P. (2001). Expectation propagation for approximate Bayesian inference. In <em>Uncertainty in artificial intelligence</em>.</p>
</div>
<div id="ref-neal1993new">
<p>Neal, R. M., &amp; Hinton, G. E. (1993). A new view of the EM algorithm that justifies incremental and other variants. In <em>Learning in graphical models</em> (pp. 355–368).</p>
</div>
<div id="ref-rue2009approximate">
<p>Rue, H., Martino, S., &amp; Chopin, N. (2009). Approximate bayesian inference for latent gaussian models by using integrated nested laplace approximations. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <em>71</em>(2), 319–392.</p>
</div>
<div id="ref-wang2012truncation">
<p>Wang, C., &amp; Blei, D. M. (2012). Truncation-free online variational inference for bayesian nonparametric models. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-wang2013variational">
<p>Wang, C., &amp; Blei, D. M. (2013). Variational inference in nonconjugate models. <em>Journal of Machine Learning Research</em>, <em>14</em>, 1005–1031.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
