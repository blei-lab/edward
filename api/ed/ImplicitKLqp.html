<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward â€“ ed.ImplicitKLqp</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/api/">API</a>
    <hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
    <a class="button u-full-width" href="/api/reference">Reference</a>
    <a class="button u-full-width" href="/api/ed/criticisms.html">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences.html">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models.html">ed.models</a>
<a class="button u-full-width" href="/api/ed/util.html">ed.util</a>
<a class="button u-full-width" href="/api/observations.html">observations</a>

    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<div itemscope="" itemtype="http://developers.google.com/ReferenceObject">
<meta itemprop="name" content="ed.ImplicitKLqp" /><meta itemprop="property" content="__init__"/><meta itemprop="property" content="build_loss_and_gradients"/><meta itemprop="property" content="finalize"/><meta itemprop="property" content="initialize"/><meta itemprop="property" content="print_progress"/><meta itemprop="property" content="run"/><meta itemprop="property" content="update"/>
</div>
<h1 id="ed.implicitklqp">ed.ImplicitKLqp</h1>
<h2 id="class-implicitklqp">Class <code>ImplicitKLqp</code></h2>
<p>Inherits From: <a href="../ed/GANInference"><code>GANInference</code></a></p>
<h3 id="aliases">Aliases:</h3>
<ul>
<li>Class <code>ed.ImplicitKLqp</code></li>
<li>Class <code>ed.inferences.ImplicitKLqp</code></li>
</ul>
<p>Defined in <a href="https://github.com/blei-lab/edward/tree/master/edward/inferences/implicit_klqp.py"><code>edward/inferences/implicit_klqp.py</code></a>.</p>
<p>Variational inference with implicit probabilistic models <span class="citation">(Tran et al., 2017)</span>.</p>
<p>It minimizes the KL divergence</p>
<p><span class="math inline">\(\text{KL}( q(z, \beta; \lambda) \| p(z, \beta \mid x) ),\)</span></p>
<p>where <span class="math inline">\(z\)</span> are local variables associated to a data point and <span class="math inline">\(\beta\)</span> are global variables shared across data points.</p>
<p>Global latent variables require <code>log_prob()</code> and need to return a random sample when fetched from the graph. Local latent variables and observed variables require only a random sample when fetched from the graph. (This is true for both <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span>.)</p>
<p>All variational factors must be reparameterizable: each of the random variables (<code>rv</code>) satisfies <code>rv.is_reparameterized</code> and <code>rv.is_continuous</code>.</p>
<h4 id="notes">Notes</h4>
<p>Unlike <code>GANInference</code>, <code>discriminator</code> takes dict's as input, and must subset to the appropriate values through lexical scoping from the previously defined model and latent variables. This is necessary as the discriminator can take an arbitrary set of data, latent, and global variables.</p>
<p>Note the type for <code>discriminator</code>'s output changes when one passes in the <code>scale</code> argument to <code>initialize()</code>.</p>
<ul>
<li>If <code>scale</code> has at most one item, then <code>discriminator</code> outputs a tensor whose multiplication with that element is broadcastable. (For example, the output is a tensor and the single scale factor is a scalar.)</li>
<li>If <code>scale</code> has more than one item, then in order to scale its corresponding output, <code>discriminator</code> must output a dictionary of same size and keys as <code>scale</code>.</li>
</ul>
<h2 id="methods">Methods</h2>
<h3 id="__init__">
<code><strong>init</strong></code>
</h3>
<pre class="python"><code>__init__(
    latent_vars,
    data=None,
    discriminator=None,
    global_vars=None
)</code></pre>
<p>Create an inference algorithm.</p>
<h4 id="args">Args:</h4>
<ul>
<li><b><code>discriminator</code></b>: function. Function (with parameters). Unlike <code>GANInference</code>, it is interpreted as a ratio estimator rather than a discriminator. It takes three arguments: a data dict, local latent variable dict, and global latent variable dict. As with GAN discriminators, it can take a batch of data points and local variables, of size <span class="math inline">\(M\)</span>, and output a vector of length <span class="math inline">\(M\)</span>.</li>
<li><b><code>global_vars</code></b>: dict of RandomVariable to RandomVariable, optional. Identifying which variables in <code>latent_vars</code> are global variables, shared across data points. These will not be encompassed in the ratio estimation problem, and will be estimated with tractable variational approximations.</li>
</ul>
<h3 id="build_loss_and_gradients">
<code>build_loss_and_gradients</code>
</h3>
<pre class="python"><code>build_loss_and_gradients(var_list)</code></pre>
<p>Build loss function</p>
<p><span class="math inline">\(-\Big(\mathbb{E}_{q(\beta)} [\log p(\beta) - \log q(\beta) ] +  \sum_{n=1}^N \mathbb{E}_{q(\beta)q(z_n\mid\beta)} [  r^*(x_n, z_n, \beta) ] \Big).\)</span></p>
<p>We minimize it with respect to parameterized variational families <span class="math inline">\(q(z, \beta; \lambda)\)</span>.</p>
<p><span class="math inline">\(r^*(x_n, z_n, \beta)\)</span> is a function of a single data point <span class="math inline">\(x_n\)</span>, single local variable <span class="math inline">\(z_n\)</span>, and all global variables <span class="math inline">\(\beta\)</span>. It is equal to the log-ratio</p>
<p><span class="math inline">\(\log p(x_n, z_n\mid \beta) - \log q(x_n, z_n\mid \beta),\)</span></p>
<p>where <span class="math inline">\(q(x_n)\)</span> is the empirical data distribution. Rather than explicit calculation, <span class="math inline">\(r^*(x, z, \beta)\)</span> is the solution to a ratio estimation problem, minimizing the specified <code>ratio_loss</code>.</p>
<p>Gradients are taken using the reparameterization trick <span class="citation">(Kingma &amp; Welling, 2014)</span>.</p>
<h4 id="notes-1">Notes</h4>
<p>This also includes model parameters <span class="math inline">\(p(x, z, \beta; \theta)\)</span> and variational distributions with inference networks <span class="math inline">\(q(z\mid x)\)</span>.</p>
<p>There are a bunch of extensions we could easily do in this implementation:</p>
<ul>
<li>further factorizations can be used to better leverage the graph structure for more complicated models;</li>
<li>score function gradients for global variables;</li>
<li>use more samples; this would require the <code>copy()</code> utility function for q's as well, and an additional loop. we opt not to because it complicates the code;</li>
<li>analytic KL/swapping out the penalty term for the globals.</li>
</ul>
<h3 id="finalize">
<code>finalize</code>
</h3>
<pre class="python"><code>finalize()</code></pre>
<p>Function to call after convergence.</p>
<h3 id="initialize">
<code>initialize</code>
</h3>
<pre class="python"><code>initialize(
    ratio_loss=&#39;log&#39;,
    *args,
    **kwargs
)</code></pre>
<p>Initialize inference algorithm. It initializes hyperparameters and builds ops for the algorithm's computation graph.</p>
<h4 id="args-1">Args:</h4>
<ul>
<li><b><code>ratio_loss</code></b>: str or fn, optional. Loss function minimized to get the ratio estimator. 'log' or 'hinge'. Alternatively, one can pass in a function of two inputs, <code>psamples</code> and <code>qsamples</code>, and output a point-wise value with shape matching the shapes of the two inputs.</li>
</ul>
<h3 id="print_progress">
<code>print_progress</code>
</h3>
<pre class="python"><code>print_progress(info_dict)</code></pre>
<p>Print progress to output.</p>
<h3 id="run">
<code>run</code>
</h3>
<pre class="python"><code>run(
    variables=None,
    use_coordinator=True,
    *args,
    **kwargs
)</code></pre>
<p>A simple wrapper to run inference.</p>
<ol style="list-style-type: decimal">
<li>Initialize algorithm via <code>initialize</code>.</li>
<li>(Optional) Build a TensorFlow summary writer for TensorBoard.</li>
<li>(Optional) Initialize TensorFlow variables.</li>
<li>(Optional) Start queue runners.</li>
<li>Run <code>update</code> for <code>self.n_iter</code> iterations.</li>
<li>While running, <code>print_progress</code>.</li>
<li>Finalize algorithm via <code>finalize</code>.</li>
<li>(Optional) Stop queue runners.</li>
</ol>
<p>To customize the way inference is run, run these steps individually.</p>
<h4 id="args-2">Args:</h4>
<ul>
<li><b><code>variables</code></b>: list, optional. A list of TensorFlow variables to initialize during inference. Default is to initialize all variables (this includes reinitializing variables that were already initialized). To avoid initializing any variables, pass in an empty list.</li>
<li><b><code>use_coordinator</code></b>: bool, optional. Whether to start and stop queue runners during inference using a TensorFlow coordinator. For example, queue runners are necessary for batch training with file readers. *args, **kwargs: Passed into <code>initialize</code>.</li>
</ul>
<h3 id="update">
<code>update</code>
</h3>
<pre class="python"><code>update(
    feed_dict=None,
    variables=None
)</code></pre>
<p>Run one iteration of optimization.</p>
<h4 id="args-3">Args:</h4>
<ul>
<li><b><code>feed_dict</code></b>: dict, optional. Feed dictionary for a TensorFlow session run. It is used to feed placeholders that are not fed during initialization.</li>
<li><b><code>variables</code></b>: str, optional. Which set of variables to update. Either &quot;Disc&quot; or &quot;Gen&quot;. Default is both.</li>
</ul>
<h4 id="returns">Returns:</h4>
<p>dict. Dictionary of algorithm-specific information. In this case, the iteration number and generative and discriminative losses.</p>
<h4 id="notes-2">Notes</h4>
<p>The outputted iteration number is the total number of calls to <code>update</code>. Each update may include updating only a subset of parameters.</p>
<div id="refs" class="references">
<div id="ref-kingma2014auto">
<p>Kingma, D., &amp; Welling, M. (2014). Auto-encoding variational Bayes. In <em>International conference on learning representations</em>.</p>
</div>
<div id="ref-tran2017deep">
<p>Tran, D., Hoffman, M. D., Saurous, R. A., Brevdo, E., Murphy, K., &amp; Blei, D. M. (2017). Deep probabilistic programming. In <em>International conference on learning representations</em>.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
