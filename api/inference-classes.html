<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Classes of Inference</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Downgrade API's header styles without explicitly modifying their type. */
  div.nine.columns h1 { text-align: left; }
  div.nine.columns h1 { font-size: 3.0rem; line-height: 1.25; letter-spacing: -.1rem; }
  div.nine.columns h2 { font-size: 2.4rem; line-height: 1.3;  letter-spacing: -.1rem; }
  div.nine.columns h3 { font-size: 2.4rem; line-height: 1.35; letter-spacing: -.08rem; }
  div.nine.columns h4 { font-size: 1.8rem; line-height: 1.5;  letter-spacing: -.05rem; }
  div.nine.columns h5 { font-size: 1.5rem; line-height: 1.6;  letter-spacing: 0; }
  @media (min-width: 550px) {
    div.nine.columns h1 { font-size: 3.6rem; }
    div.nine.columns h2 { font-size: 3.0rem; }
    div.nine.columns h3 { font-size: 3.0rem; }
    div.nine.columns h4 { font-size: 2.5rem; }
    div.nine.columns h5 { font-size: 1.5rem; }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/api/">API</a>
    <hr style="margin-top: 1rem; margin-bottom: 1.5rem;"/>
    <a class="button u-full-width" href="/api/reference">Reference</a>
    <a class="button u-full-width" href="/api/ed/criticisms">ed.criticisms</a>
<a class="button u-full-width" href="/api/ed/inferences">ed.inferences</a>
<a class="button u-full-width" href="/api/ed/models">ed.models</a>
<a class="button u-full-width" href="/api/ed/util">ed.util</a>
<a class="button u-full-width" href="/api/observations">observations</a>

    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">
<h1 id="api-and-documentation">API and Documentation</h1>
<div class="row" style="padding-bottom: 5%">
<div class="row" style="padding-bottom: 1%">
<a class="button3" href="/api/data">Data</a>
<a class="button3" href="/api/model">Model</a>
<a class="button3 button-primary" href="/api/inference">Inference</a>
<a class="button3" href="/api/criticism">Criticism</a>
</div>
<div class="row">
<a class="button4 button-primary" href="/api/inference-classes">Classes</a>
<a class="button4" href="/api/inference-compositionality">Compositionality</a>
<a class="button4" href="/api/inference-data-subsampling">Data Subsampling</a>
<a class="button4" href="/api/inference-development">Development</a>
</div>
</div>
<h3 id="classes-of-inference">Classes of Inference</h3>
<p>Inference is broadly classified under three classes: variational inference, Monte Carlo, and exact inference. We highlight how to use inference algorithms from each class.</p>
<p>As an example, we assume a mixture model with latent mixture assignments <code>z</code>, latent cluster means <code>beta</code>, and observations <code>x</code>: <span class="math display">\[p(\mathbf{x}, \mathbf{z}, \beta)
=
\text{Normal}(\mathbf{x} \mid \beta_{\mathbf{z}}, \mathbf{I})
~
\text{Categorical}(\mathbf{z}\mid \pi)
~
\text{Normal}(\beta\mid \mathbf{0}, \mathbf{I}).\]</span></p>
<h3 id="variational-inference">Variational Inference</h3>
<p>In variational inference, the idea is to posit a family of approximating distributions and to find the closest member in the family to the posterior <span class="citation">(Jordan, Ghahramani, Jaakkola, &amp; Saul, 1999)</span>. We write an approximating family, <span class="math display">\[\begin{aligned}
q(\beta;\mu,\sigma) &amp;= \text{Normal}(\beta; \mu,\sigma), \\[1.5ex]
q(\mathbf{z};\pi) &amp;= \text{Categorical}(\mathbf{z};\pi),\end{aligned}\]</span> using TensorFlow variables to represent its parameters <span class="math inline">\(\lambda=\{\pi,\mu,\sigma\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Categorical, Normal

qbeta = Normal(loc=tf.Variable(tf.zeros([K, D])),
               scale=tf.exp(tf.Variable(tf.zeros[K, D])))
qz = Categorical(logits=tf.Variable(tf.zeros[N, K]))

inference = ed.VariationalInference({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Given an objective function, variational inference optimizes the family with respect to <code>tf.Variable</code>s.</p>
<p>Specific variational inference algorithms inherit from the <code>VariationalInference</code> class to define their own methods, such as a loss function and gradient. For example, we represent MAP estimation with an approximating family (<code>qbeta</code> and <code>qz</code>) of <code>PointMass</code> random variables, i.e., with all probability mass concentrated at a point.</p>
<pre class="python" language="Python"><code>from edward.models import PointMass

qbeta = PointMass(params=tf.Variable(tf.zeros([K, D])))
qz = PointMass(params=tf.Variable(tf.zeros(N)))

inference = ed.MAP({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p><code>MAP</code> inherits from <code>VariationalInference</code> and defines a loss function and update rules; it uses existing optimizers inside TensorFlow.</p>
<h3 id="monte-carlo">Monte Carlo</h3>
<p>Monte Carlo approximates the posterior using samples <span class="citation">(Robert &amp; Casella, 1999)</span>. Monte Carlo is an inference where the approximating family is an empirical distribution, <span class="math display">\[\begin{aligned}
q(\beta; \{\beta^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\beta, \beta^{(t)}), \\[1.5ex]
q(\mathbf{z}; \{\mathbf{z}^{(t)}\})
&amp;= \frac{1}{T}\sum_{t=1}^T \delta(\mathbf{z}, \mathbf{z}^{(t)}).\end{aligned}\]</span> The parameters are <span class="math inline">\(\lambda=\{\beta^{(t)},\mathbf{z}^{(t)}\}\)</span>.</p>
<pre class="python" language="Python"><code>from edward.models import Empirical

T = 10000  # number of samples
qbeta = Empirical(params=tf.Variable(tf.zeros([T, K, D]))
qz = Empirical(params=tf.Variable(tf.zeros([T, N]))

inference = ed.MonteCarlo({beta: qbeta, z: qz}, data={x: x_train})</code></pre>
<p>Monte Carlo algorithms proceed by updating one sample <span class="math inline">\(\beta^{(t)},\mathbf{(z)}^{(t)}\)</span> at a time in the empirical approximation. Markov chain Monte Carlo does this sequentially to update the current sample (index <span class="math inline">\(t\)</span> of <code>tf.Variable</code>s) conditional on the last sample (index <span class="math inline">\(t-1\)</span> of <code>tf.Variable</code>s). Specific Monte Carlo samplers determine the update rules; they can use gradients such as in Hamiltonian Monte Carlo <span class="citation">(Neal, 2011)</span> and graph structure such as in sequential Monte Carlo <span class="citation">(Doucet, De Freitas, &amp; Gordon, 2001)</span>.</p>
<h3 id="non-bayesian-methods">Non-Bayesian Methods</h3>
<p>As a library for probabilistic modeling (not necessarily Bayesian modeling), Edward is agnostic to the paradigm for inference. This means Edward can use frequentist (population-based) inferences, strictly point estimation, and alternative foundations for parameter uncertainty.</p>
<p>For example, Edward supports non-Bayesian methods such as generative adversarial networks (GANs) <span class="citation">(Goodfellow et al., 2014)</span>. For more details, see the <a href="/tutorials/gan">GAN tutorial</a>.</p>
<p>In general, we think opening the door to non-Bayesian approaches is a crucial feature for probabilistic programming. This enables advances in other fields such as deep learning to be complementary: all is in service for probabilistic models and thus it makes sense to combine our efforts.</p>
<h3 id="exact-inference">Exact Inference</h3>
<p>In order to uncover conjugacy relationships between random variables (if they exist), we use symbolic algebra on nodes in the computational graph. Users can then integrate out variables to automatically derive classical Gibbs <span class="citation">(Gelfand &amp; Smith, 1990)</span>, mean-field updates <span class="citation">(Bishop, 2006)</span>, and exact inference.</p>
<p>For example, can calculate a conjugate posterior analytically by using the <code>ed.complete_conditional</code> function:</p>
<pre class="python" language="Python"><code>from edward.models import Bernoulli, Beta

# Beta-Bernoulli model
pi = Beta(1.0, 1.0)
x = Bernoulli(probs=pi, sample_shape=10)

# Beta posterior; it conditions on the sample tensor associated to x
pi_cond = ed.complete_conditional(pi)

# Generate samples from p(pi | x = NumPy array)
sess.run(pi_cond, {x: np.array([0, 1, 0, 0, 0, 0, 0, 0, 0, 1])})</code></pre>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-bishop2006pattern">
<p>Bishop, C. M. (2006). <em>Pattern recognition and machine learning</em>. Springer New York.</p>
</div>
<div id="ref-doucet2001introduction">
<p>Doucet, A., De Freitas, N., &amp; Gordon, N. (2001). An introduction to sequential Monte Carlo methods. In <em>Sequential monte carlo methods in practice</em> (pp. 3–14). Springer.</p>
</div>
<div id="ref-gelfand1990sampling">
<p>Gelfand, A. E., &amp; Smith, A. F. (1990). Sampling-based approaches to calculating marginal densities. <em>Journal of the American Statistical Association</em>, <em>85</em>(410), 398–409.</p>
</div>
<div id="ref-goodfellow2014generative">
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-jordan1999introduction">
<p>Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. <em>Machine Learning</em>, <em>37</em>(2), 183–233.</p>
</div>
<div id="ref-neal2011mcmc">
<p>Neal, R. M. (2011). MCMC using Hamiltonian dynamics. <em>Handbook of Markov Chain Monte Carlo</em>.</p>
</div>
<div id="ref-robert1999monte">
<p>Robert, C. P., &amp; Casella, G. (1999). <em>Monte carlo statistical methods</em>. Springer.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
