\title{Tutorials}

\subsection{Tutorials}

Edward provides a testbed for rapid experimentation and research with
probabilistic models. Here we show how to apply this process for
diverse learning tasks.

\href{supervised-regression}{Bayesian linear regression} \\
A fundamental model for supervised learning.

\href{supervised-classification}{Gaussian process classification} \\
Learning a distribution over functions for supervised classification.

\href{unsupervised}{Mixture models} \\
Unsupervised learning by clustering data points.

\href{latent-space-models}{Latent space models} \\
Analyzing connectivity patterns in neural data.

\href{mixture-density-network}{Mixture density networks} \\
A neural density estimator for solving inverse problems.

\href{gan}{Generative adversarial networks} \\
Building a deep generative model of MNIST digits.

\href{decoder}{Probabilistic decoder} \\
A model of latent codes in information theory.

\href{inference-networks}{Inference networks} \\
How to amortize computation for training and testing models.

\href{bayesian-neural-network}{Bayesian neural network} \\
Bayesian analysis with neural networks.

If you're interested in contributing a tutorial, checking out the
\href{/contributing}{contributing page}.
For more background and notation, see the pages below.
\begin{itemize}
  \item \href{model}{Probabilistic models}
  \item \href{inference}{Inference of probabilistic models}
  \begin{itemize}
   \item \href{variational-inference}{Variational inference}
   \item \href{klqp}{$\text{KL}(q\|p)$ minimization}
   \item \href{klpq}{$\text{KL}(p\|q)$ minimization}
   \item \href{map}{Maximum a posteriori estimation}
   \item \href{map-laplace}{Laplace approximation}
  \end{itemize}
  \item \href{criticism}{Model criticism}
\end{itemize}
