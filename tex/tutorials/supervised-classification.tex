\title{Supervised learning (Classification)}

\subsection{Supervised learning (Classification)}

In supervised learning, the task is to infer hidden structure from
labeled data, comprised of training examples $\{(x_n, y_n)\}$.
Classification means the output $y$ takes discrete values.

We demonstrate how to do this in Edward with an example.
The script is available
\href{https://github.com/blei-lab/edward/blob/master/examples/tf_gp_classification.py}
{here}.


\subsubsection{Data}

Use 25 data points from the \href
{https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/crabs.html}
{crabs data set}.
\begin{lstlisting}[language=Python]
df = np.loadtxt('data/crabs_train.txt', dtype='float32', delimiter=',')
df[df[:, 0] == -1, 0] = 0  # replace -1 label with 0 label

N = 25  # number of data points
D = df.shape[1] - 1  # number of features

subset = np.random.choice(df.shape[0], N, replace=False)
X_train = df[subset, 1:]
y_train = df[subset, 0]
\end{lstlisting}


\subsubsection{Model}

Posit the model as Gaussian process classification. For more details on the
model, see the
\href{/tutorials/gp-classification}
{Gaussian process classification tutorial}.

Here we build the model in Edward.
\begin{lstlisting}[language=Python]
def kernel(x):
  mat = []
  for i in range(N):
    mat += [[]]
    xi = x[i, :]
    for j in range(N):
      if j == i:
        mat[i] += [multivariate_rbf(xi, xi)]
      else:
        xj = x[j, :]
        mat[i] += [multivariate_rbf(xi, xj)]

    mat[i] = tf.pack(mat[i])

  return tf.pack(mat)


X = tf.placeholder(tf.float32, [N, D])
f = MultivariateNormalFull(mu=tf.zeros(N), sigma=kernel(X))
y = Bernoulli(logits=f)
\end{lstlisting}


\subsubsection{Inference}

Perform variational inference.
Define the variational model to be a fully factorized normal
\begin{lstlisting}[language=Python]
qf = Normal(mu=tf.Variable(tf.random_normal([N])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))
\end{lstlisting}

Run variational inference for 500 iterations.
\begin{lstlisting}[language=Python]
data = {X: X_train, y: y_train}
inference = ed.KLqp({f: qf}, data)
inference.run(n_iter=500)
\end{lstlisting}
In this case
\texttt{KLqp} defaults to minimizing the
$\text{KL}(q\|p)$ divergence measure using the reparameterization
gradient.
For more details on inference, see the \href{/tutorials/klqp}{$\text{KL}(q\|p)$ tutorial}.
(This example happens to be slow because evaluating and inverting full
covariances in Gaussian processes happens to be slow.)


%\subsubsection{Criticism}
