\title{Mixture of Gaussians}

\subsection{Mixture of Gaussians}

A mixture model is a model typically used for clustering.
It assigns a mixture component to each data point, and this mixture component
determines the distribution that the data point is generated from. A
mixture of Gaussians uses Gaussian distributions to generate this data
\citep{bishop2006pattern}.

For a set of $N$ data points,
the likelihood of each observation $\mathbf{x}_n$ is
\begin{align*}
  p(\mathbf{x}_n \mid \pi, \mu, \sigma)
  &=
  \sum_{k=1}^K \pi_k \, \mathcal{N}(\mathbf{x}_n ; \mu_k, \sigma_k).
\end{align*}
The latent variable $\pi$ is a $K$-dimensional probability vector
which mixes individual Gaussian distributions, each
characterized by mean $\mu_k$ and variance $\sigma_k$.

Define the prior on $\pi\in[0,1]$ such that $\sum_{k=1}^K\pi_k=1$ to be
\begin{align*}
  p(\pi)
  &=
  \text{Dirichlet}(\pi \;;\; \alpha \mathbf{1}_{K}).
\end{align*}

Define the prior on each component $\mathbf{\mu}_k\in\mathbb{R}^D$ to be
\begin{align*}
  p(\mathbf{\mu}_k)
  &=
  \mathcal{N}(\mathbf{\mu}_k \;;\; 0, \sigma^2\mathbf{I}).
\end{align*}

Define the prior on each component $\mathbf{\sigma}_k\in\mathbb{R}^D$ to be
\begin{align*}
  p(\mathbf{\sigma}_k)
  &=
  \text{InvGamma}(\mathbf{\sigma}_k \;;\; a, b).
\end{align*}

Let's build the model in Edward using TensorFlow. This simply requires
writing down the model's log joint density,
\begin{align*}
  \log p(\pi) +
  \Big[ \sum_{k=1}^K \log p(\mathbf{\mu}_k) + \log
  p(\mathbf{\sigma}_k) \Big] +
  \sum_{n=1}^N \log p(\mathbf{x}_n \mid \pi, \mu, \sigma).
\end{align*}
Writing
the model's log-likelihood can be tricky:
\begin{align*}
  \sum_{n=1}^N \log p(\mathbf{x}_n \mid \pi, \mu, \sigma)
  &=
  \sum_{n=1}^N \log \sum_{k=1}^K \pi_k \, \mathcal{N}(\mathbf{x}_n ;
  \mu_k, \sigma_k).
\end{align*}
To prevent numerical instability, we'd like to work on the log-scale
when calculating densities,
\begin{align*}
  \sum_{n=1}^N \log p(\mathbf{x}_n \mid \pi, \mu, \sigma)
  &=
  \sum_{n=1}^N \log \sum_{k=1}^K \exp\Big(
  \log \pi_k + \log \mathcal{N}(\mathbf{x}_n ; \mu_k, \sigma_k)\Big).
\end{align*}
This expression involves a log sum exp operation, which is
numerically unstable as exponentiation will often lead to one value
dominating the rest. Therefore we use the log-sum-exp trick,
which is based on the identity
\begin{align*}
  \mathbf{x}_{\mathrm{max}}
  &=
  \arg\max \mathbf{x},
  \\
  \log \sum_i \exp(\mathbf{x}_i)
  &=
  \log \Big(\exp(\mathbf{x}_{\mathrm{max}}) \sum_i \exp(\mathbf{x}_i -
  \mathbf{x}_{\mathrm{max}})\Big)
  \\
  &=
  \mathbf{x}_{\mathrm{max}} + \log \sum_i \exp(\mathbf{x}_i -
  \mathbf{x}_{\mathrm{max}}).
\end{align*}
Subtracting the maximum value before taking the log-sum-exp leads to
more numerically stable output.
\begin{lstlisting}[language=Python]
class MixtureGaussian:
  """
  Mixture of Gaussians

  p(x, z) = [ prod_{n=1}^N sum_{k=1}^K pi_k N(x_n; mu_k, sigma_k) ]
            [ prod_{k=1}^K N(mu_k; 0, cI) Inv-Gamma(sigma_k; a, b) ]
            Dirichlet(pi; alpha)

  where z = {pi, mu, sigma} and for known hyperparameters a, b, c, alpha.

  Parameters
  ----------
  K : int
    Number of mixture components.
  D : float, optional
    Dimension of the Gaussians.
  """
  def __init__(self, K, D):
    self.K = K
    self.D = D
    self.n_vars = (2 * D + 1) * K

    self.a = 1.0
    self.b = 1.0
    self.c = 3.0
    self.alpha = tf.ones([K])

  def log_prob(self, xs, zs):
    """Return scalar, the log joint density log p(xs, zs)."""
    x = xs['x']
    pi, mus, sigmas = zs['pi'], zs['mu'], zs['sigma']
    log_prior = dirichlet.logpdf(pi, self.alpha)
    log_prior += tf.reduce_sum(norm.logpdf(mus, 0.0, self.c))
    log_prior += tf.reduce_sum(invgamma.logpdf(sigmas, self.a, self.b))

    # log-likelihood is
    # sum_{n=1}^N log sum_{k=1}^K exp( log pi_k + log N(x_n; mu_k, sigma_k) )
    # Create a K x N matrix, whose entry (k, n) is
    # log pi_k + log N(x_n; mu_k, sigma_k).
    N = get_dims(x)[0]
    matrix = []
    for k in range(self.K):
      matrix += [tf.ones(N) * tf.log(pi[k]) +
                 multivariate_normal_diag.logpdf(x,
                 mus[(k * self.D):((k + 1) * self.D)],
                 sigmas[(k * self.D):((k + 1) * self.D)])]

    matrix = tf.pack(matrix)
    # log_sum_exp() along the rows is a vector, whose nth
    # element is the log-likelihood of data point x_n.
    vector = log_sum_exp(matrix, 0)
    # Sum over data points to get the full log-likelihood.
    log_lik = tf.reduce_sum(vector)

    return log_prior + log_lik


model = MixtureGaussian(K=2, D=2)
\end{lstlisting}

We experiment with this model using variational inference in the
\href{/tutorials/unsupervised}{unsupervised learning} tutorial.
Example scripts using this model can found
\href{https://github.com/blei-lab/edward/blob/master/examples/tf_mixture_gaussian_map.py}
{here with MAP estimation} and
\href{https://github.com/blei-lab/edward/blob/master/examples/tf_mixture_gaussian_laplace.py}
{here with the Laplace approximation}.

\subsubsection{References}\label{references}
