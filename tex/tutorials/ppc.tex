\title{Posterior predictive checks}

\subsection{Posterior predictive checks}

Posterior predictive checks (PPCs)
analyze the degree to which data generated from the model deviate from
data generated from the true distribution. They can be used either
numerically to quantify this degree, or graphically to visualize this
degree. PPCs can be thought of as a probabilistic generalization of
point-based evaluations
\citep{box1980sampling,rubin1984bayesianly,meng1994posterior,gelman1996posterior}.

PPCs focus on the posterior predictive distribution
\begin{align*}
  p(x_\text{new} \mid x)
  &=
  \int
  p(x_\text{new} \mid z)
  p(z \mid x)
  \text{d} z.
\end{align*}
The model's posterior predictive can be used to generate new data
given past observations and can also make predictions on new data
given past observations.
It is formed by calculating the likelihood of the new data, averaged
over every set of latent variables according to the posterior
distribution.

The simplest PPC works by applying a test statistic on new data
generated from the posterior predictive, such as
$T(x_\text{new}) = \max(x_\text{new})$.  Applying $T(x_\text{new})$ to
new data over many data replications induces a distribution. We compare
this distribution to the test statistic applied to the real data $T(x)$.

\includegraphics{/images/ppc.png}

In the figure, $T(x)$ falls in a low probability region of this
reference distribution. This indicates that the model fits the data
poorly according to this check; this suggests an area of improvement
for the model.

More generally, the test statistic can also be a function of the
model's latent variables $T(x, z)$, known as a discrepancy function.
Examples of discrepancy functions are the metrics used for point-based
evaluation. We can now interpret the point-based evaluation as a
special case of PPCs: it simply calculates $T(x, z)$ over the real
data and without a reference distribution in mind. A reference
distribution allows us to make probabilistic statements about the
point, in reference to an overall distribution.

\subsubsection{Implementation}

To evaluate inferred models, we first form the posterior
predictive distribution. A helpful utility function for this is
\texttt{copy}. For example,
assume the model defines a likelihood \texttt{x} connected to a prior
\texttt{z}. The posterior predictive distribution is
\begin{lstlisting}[language=Python]
x_post = ed.copy(x, {z: qz})
\end{lstlisting}
Here, we copy the likelihood node \texttt{x} in the graph and replace dependence
on the prior \texttt{z} with dependence on the inferred posterior \texttt{qz}.

The \texttt{ed.ppc()} method provides a scaffold for studying
various discrepancy functions.
\begin{lstlisting}[language=Python]
def T(xs, zs):
  return tf.reduce_mean(xs[x_post])

ed.ppc(T, data={x_post: x_train})
\end{lstlisting}
The discrepancy can also take latent variables as input, which we pass
into the PPC.
\begin{lstlisting}[language=Python]
def T(xs, zs):
  return tf.reduce_mean(tf.cast(zs['z'], tf.float32))

ppc(T, data={y_post: y_train, x_ph: x_train},
    latent_vars={'z': qz, 'beta': qbeta})
\end{lstlisting}

See the \href{/api/criticism}{criticism API} for further details.

PPCs are an excellent tool for revising models, simplifying or
expanding the current model as one examines how well it fits the data.
They are inspired by prior checks and classical hypothesis
testing, under the philosophy that models should be
criticized under the frequentist perspective of large sample
assessment.

PPCs can also be applied to tasks such as hypothesis testing, model
comparison, model selection, and model averaging.  It's important to
note that while they can be applied as a form of Bayesian hypothesis
testing, hypothesis testing is generally not recommended: binary
decision making from a single test is not as common a use case as one
might believe. We recommend performing many PPCs to get a holistic
understanding of the model fit.

\subsubsection{Implementation (model wrappers)}

For model wrappers, Edward implements PPCs through the
\texttt{sample_likelihood()} method in the class object. This method
samples a dataset
$x_{\text{new}}\mid z\sim p(x_{\text{new}}\mid z)$ from the
model likelihood given a set of latent variables.
\begin{lstlisting}[language=Python]
class BetaBernoulli:
  ...
  def sample_likelihood(self, zs):
    """x | p ~ p(x | p)"""
    return {'x': bernoulli.sample(p=tf.ones(10) * zs['p'])}
\end{lstlisting}

An examples of a PPC for model wrappers is
\begin{lstlisting}[language=Python]
def T(xs, zs):
  return tf.reduce_mean(xs['x'])

ed.ppc(T, data={'x': x_train}, latent_vars={'z': qz}, model_wrapper=model)
\end{lstlisting}
For model wrappers, the latent variables must always be
passed in regardless of whether \texttt{T} is a function of them.

\subsubsection{References}\label{references}

