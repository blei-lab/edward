\title{Design Philosophy}

\subsection{Design Philosophy}

\textbf{Edward} serves two purposes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  to serve as a foundation for research;
\item
  to serve as a unified library for modeling, inference, and criticism.
\end{enumerate}

As a research tool, the code base provides a testbed for fast
experimentation. For example, to evaluate new inference algorithms,
the experiments only need to be written once and we can simply swap
the inference with the newly proposed algorithm. This idea applies to
all components of probabilistic modeling: we can leverage the built-in
inference algorithms to develop new complex models, or use the
built-in models and inference to develop new criticism techniques.

As an applied tool, Edward supports a wide variety of settings,
ranging from classical hierarchical models on small data sets to
complex deep probabilistic models on large data sets. With
\href{https://www.tensorflow.org}{TensorFlow} as a backend, Edward can
leverage features such as computational graphs, distributed training,
and CPU/GPU integration to deploy probabilistic modeling at scale.

\subsection{Related Software}\label{related-software}

There are several notable themes in Edward.

\textbf{Probabilistic programming.}
There has been outstanding work on programming languages which specify
broad classes of probabilistic models, or probabilistic
programs. Among these include
\href{http://projects.csail.mit.edu/church/wiki/Church}{Church},
\href{http://probcomp.csail.mit.edu/venture/}{Venture},
\href{http://www.robots.ox.ac.uk/~fwood/anglican/literature/index.html}{Anglican},
\href{http://mc-stan.org}{Stan}, and
\href{http://webppl.org}{WebPPL}.
The most important distinction in Edward comes from our motivation.
We are interested in deploying probabilistic models to a myriad of
real world applications, ranging from the size of data and
data structure, such as large text corpora or many brief audio signals,
to the size of model and class of models, such as small nonparametric
models or deep generative models.
%As one consequence,
%Edward focuses not only on the abstraction of model and inference, but
%across what we view encapsulates all data analysis: modeling, inference,
%and criticism.
Thus Edward is built with efficient (and possibly distributed)
training in mind, beyond the setting where all computing is on CPUs
or where data must fit in memory.
%As another, we do not prioritize development of
%modeling languages. For example, we instead leverage Stan's
%language as an easy way to specify models in Edward.


\textbf{Black box inference.}
Black box algorithms are typically Monte Carlo methods, and
by design make very few assumptions about the model.
Our motivation as outlined above presents a new set of
challenges in both inference research and their software design.
As one consequence, we focus on variational inference.
As a second consequence, we encourage active research
on inference by providing a class hierarchy of inference algorithms.
As a third consequence, our inference
algorithms aim to take advantage of as much structure as possible from
the model. Edward supports all types of inference, whether they
be black box or model-specific.

\textbf{Computational frameworks.}
There are many computational frameworks, primarily built for deep
learning: as of this date, this includes
\href{https://www.tensorflow.org}{TensorFlow},
\href{http://deeplearning.net/software/theano/}{Theano},
\href{http://torch.ch}{Torch},
\href{https://github.com/NervanaSystems/neon}{neon},
\href{http://rll.berkeley.edu/cgt/}{Computational Graph Toolkit}, and
\href{https://github.com/stan-dev/math}{Stan Math Library}. These are
incredible tools which Edward employs as a backend. In
terms of abstraction, Edward sits at one level higher. For example,
we chose to use TensorFlow among the bunch due to Python as our language
of choice for machine learning research and as an investment in Google's
massive engineering.

\textbf{High-level deep learning libraries.}
Neural network libraries such as
\href{https://github.com/fchollet/keras}{Keras} and
\href{https://github.com/Lasagne/Lasagne}{Lasagne} are at a similar
abstraction level as us, but they are primarily interested in
parameterizing complicated functions for supervised learning on large
datasets. We are interested in probabilistic models which apply
to a wide array of learning tasks, and which can have both
complicated likelihood and complicated priors (neural networks are an
option but not a necessity). Therefore our goals are orthogonal and in
fact mutually benefit each other. For example, we use Keras'
abstraction as a way to easily specify models parameterized by deep
neural networks.
