\title{Data Subsampling}

{{navbar}}

\subsubsection{Data Subsampling}

Batch training is desirable for scalability. In the data section, we
described how to feed batches of data at a time.  In Bayesian
inference we have to deal with the additional problem of local versus
global latent variables.  In this API, the modeler never explicitly
writes down plates, so we can't tell which subset of the local latent
variables are relevant to any batch (TODO although we could prbably do
it internally, with batch\_shape/event\_shape; this also comes up in the
bijector class).

Our key idea is to enable manual control of latent variable updates
during inference. We illustrate below according to different settings
of scalability.

\textbf{Full graphs.}
Suppose the probability model and variational model both fit in
memory.
\begin{lstlisting}[language=Python]
N = 10000000 # num data points
M = 5 # mini-batch size

# p(y, z, beta) = p(beta) prod_{n=1}^N p(z_n | beta) p(y_n | z_n, beta)
beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for n in range(N)]))
y = RandomVariable(par_1=z, par_2=tf.pack([beta for n in range(N)]))

# q(beta, z) = q(beta; lambda) prod_{n=1}^N q(z_n | beta; gamma_n)
qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
qz = RandomVariable(par=f(tf.pack([qbeta for n in range(N)]),
                          tf.Variable(tf.random_normal(N)))

y_ph = tf.placeholder(tf.float32, [M])
idx_ph = tf.placeholder(tf.int32, [M])
z_scale = tf.placeholder(tf.float32, [])
inference = ed.VI({beta: qbeta, tf.gather(z, idx_ph): tf.gather(qz, idx_ph)},
                  data={y: y_ph},
                  scale={z: z_scale})
inference.initialize()
for t in range(10000):
  y_batch = next_batch(size=M)
  local_idx = np.arange(t*M, (t+1)*M) % N
  for _ in range(10):
    inference.update(random_vars={tf.gather(z, idx_ph): tf.gather(qz, idx_ph)},
                     feed_dict={y_ph: y_batch, z_scale: 1.0, idx_ph: local_idx})
  inference.update(random_vars={beta: qbeta},
                   feed_dict={y_ph: y_batch, z_scale: float(N)/M,
                              idx_ph: local_idx})
\end{lstlisting}
There is a change to the usual inference:
\begin{itemize}
\item
  We do separate updates of \texttt{z} and \texttt{beta}. Because we only update a
  batch of data points and their associated latent variables
  (\texttt{z[local_idx]}), we must scale computation of \texttt{z} when updating
  \texttt{beta}, so it is as if we had seen the full data set \texttt{scale} many
  times. This enables unbiased updates.
\end{itemize}

This approach is scalable in that computational complexity is
independent of the size of the data set. The size of parameters
however grows with the size of data.

\textbf{Subgraphs.}
Suppose the full probability model and variational model no longer fit
in memory.
\begin{lstlisting}[language=Python]
N = 10000000 # num data points
M = 5 # mini-batch size

# p(y, z, beta) = p(beta) prod_{n=1}^N p(z_n | beta) p(y_n | z_n, beta)
# Define a subgraph, p(beta) prod_{m=1}^M p(z_m | beta) p(y_m | z_m, beta)
beta = RandomVariable(par=tf.ones(5))
z = RandomVariable(par=tf.pack([beta for m in range(M)]))
y = RandomVariable(par_1=z par_2=tf.pack([beta for m in range(M)]))

# q(beta, z) = q(beta; lambda) prod_{n=1}^N q(z_n | beta; gamma_n)
# Define a subgraph, q(beta; lambda) prod_{m=1}^M q(z_m | beta; gamma_m)
qbeta = RandomVariable(par=tf.Variable(tf.random_normal(5)))
with tf.variable_scope("qz"):
  qz = RandomVariable(par=f(tf.pack([qbeta for m in range(M)]),
                            tf.Variable(tf.random_normal(M)))

y_ph = tf.placeholder(tf.float32, [M])
z_scale = tf.placeholder(tf.float32, [])
inference = ed.VI({beta: qbeta, z: qz}, data={y: y_ph}, scale={z: z_scale})
inference.initialize()
for t in range(10000):
  y_batch = next_batch(size=M)
  for _ in range(10):
    inference.update(random_vars={z: qz},
                     feed_dict={y_ph: y_batch, z_scale: 1.0})
  inference.update(random_vars={beta: qbeta},
                   feed_dict={y_ph: y_batch, z_scale: float(N)/M})
  tf.initialize_variables(tf.get_collection("qz"))
\end{lstlisting}
There is a change to the usual probability model:
\begin{itemize}
\item
  We use \texttt{M} instead of \texttt{N}. This defines a subgraph of the full
  probability model, representing only the piece relevant to a batch of
  data during inference. Conceptually \texttt{M} is the plate size of the
  graphical model.
\end{itemize}

There is a change to the usual variational model:
\begin{itemize}
\item
  We use \texttt{M} instead of \texttt{N}. This defines a subgraph of the full
  variational model, representing only the piece relevant to a batch of
  data during inference. Conceptually \texttt{M} is the plate size of the
  graphical model.
\end{itemize}

There is a change to the usual inference:
\begin{itemize}
\item
  We perform updates but no longer require any local indexing because
  the subgraphs are already all local.
\item
  We reset the parameters for \texttt{qz} because we do inference on a new
  set of local variational factors for each batch. The command
  reinitializes all \texttt{tf.Variables()} relevant to \texttt{qz}.

  If we care about the local parameters, we can save them. For
  example, if we want to do computation over multiple epochs, and we
  don't have enough memory to store all parameters, then we can write
  the parameters to disk (at the end of the loop) and re-read them.
\item
  We can amortize computation using an inference network, which easily
  applies to this setting based on a global parameterization of
  \texttt{qbeta} and \texttt{qz}. In such a case, no in-place resetting of subgraph
  parameters are necessary.
\end{itemize}

This approach is the most scalable: both computational complexity and
memory complexity is independent of the size of the data set.

Sometimes resetting parameters is not sufficient, such as when \texttt{qz} may
have a different distributional form for each data point. In this
case, we can create random variables on the fly using
local and global inference instantiations.
\begin{lstlisting}[language=Python]
inference = ed.VI({beta: qbeta}, data={y: y_ph, z: qz},
                  scale={z: float(N)/M})
global_vi.initialize()
for _ in range(10000):
  y_batch = next_batch(size=M)
  qz = RandomVariable(M, par=qbeta)
  local_vi = ed.VI({z: qz}, data={y_ph: y_batch, beta: qbeta})
  for _ in range(10):
    local_vi.update()

  global_vi.update(feed_dict={y_ph: y_batch})
\end{lstlisting}

\begin{lstlisting}[language=Python]
global_vi = ed.VI({beta: qbeta}, data={y: y_ph, z: qz}, scale={z: float(N)/M})
global_vi.initialize()
for _ in range(10000):
  y_batch = next_batch(size=M)
  qz = RandomVariable(M, par=qbeta)
  local_vi = ed.VI({z: qz}, data={y: y_batch, beta: qbeta})
  for _ in range(10):
    local_vi.update()

  global_vi.update(feed_dict={y_ph: y_batch})
\end{lstlisting}
\begin{itemize}
\item
  We define a global VI object and a local VI object. Each abstractly
  defines inference over different distributions (one does $q(\beta)
  \approx p(\beta \mid x)$; the other does $\prod q(z_n) \approx \prod
  p(z_n \mid \beta, x))$. We create the global VI object outside the loop
  and proceed to update it per iteration. We create the local VI
  object inside the loop and update it only once.
\item
  This construct is very general. We can arbitrarily compose inference
  algorithms to infer different conditional distributions within the
  full posterior. For example, instead of using both VI for both
  inferring $p(\beta \mid x)$ and $\prod p(z_n \mid \beta, x)$, we can do
  MCMC to exactly infer only one of these.
\item
  The TensorFlow graph is ``unrolled'', in the sense that a new node is
  being added at each step of inference.
\end{itemize}

{{autogenerated}}
