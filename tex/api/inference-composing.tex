\title{Composing Inference}

{{navbar}}

\subsubsection{Composing Inference}

Core to this language for inference is composability. Composability
enables fine-grained control of latent variable updates, where we can
leverage different algorithms for each latent variable.

\subsubsection{Hybrid algorithms}

Hybrid algorithms such as EM can be represented as a composition of two
algorithms.
\begin{lstlisting}[language=Python]
qbeta = PointMass()
qz = RandomVariable()
inference_e = ed.KLqp({z: qz}, data={x: x_train, beta: qbeta})
inference_m = ed.MAP({beta: qbeta}, data={x: x_train, z: qz})

for _ in range(10000):
  inference_e.update()
  inference_m.update()
\end{lstlisting}
There are many examples of hybrid algorithms:
exact EM for exponential families,
Monte Carlo EM,
pseudo-marginal and ABC methods \citep{andrieu2009pseudo},
structured variational auto-encoders \citep{johnson2016composing},
variational inference with Gibbs sampling \citep{wang2012truncation},
and Laplace variational inference \citep{wang2013variational}.

% One can also do this without passing in the random variables
% explicitly as part of the data, restricting the inner integral to be
% approximated with one sample.
% \begin{lstlisting}[language=Python]
% qbeta = PointMass()
% qz = RandomVariable()

% beta_ph = tf.placeholder(tf.float32)
% z_ph = tf.placeholder(tf.float32)
% inference_e = ed.KLqp({z: qz}, data={x: x_train, beta: beta_ph})
% inference_m = ed.MAP({beta: qbeta}, data={x: x_train, z: z_ph})

% for _ in range(10000):
%   inference_e.update(feed_dict={beta_ph: qbeta.sample()})
%   inference_m.update(feed_dict={z_ph: qz.sample()})
% \end{lstlisting}
% Note this is equivalent for the E-step, as the $q(\beta)$ is a point mass
% anyways. In general, passing the whole random varriable vs a sample
% are different different: for example, in BBVI, the latter will only
% add the prior and likelihood to the ELBO; the former will additionally
% include the variational log-density to the ELBO;.

\subsubsection{Message passing algorithms}

We can also consider message passing \citep{koller2009probabilistic}.
Message passing algorithms operate on the posterior distribution using
local inferences. With TensorFlow's distributed training,
composability enables \emph{distributed} message passing over a
cluster with many workers.

Let's consider Gibbs sampling. We define the complete conditional
where the latent variables are conditioned on the last sample from the
Gibbs algorithm.
\begin{lstlisting}[language=Python]
t = tf.Variable(0, trainable=False)
increment_t = t.assign_add(1)

inference_z1 = ed.ConjugateInference(
    {z1: qz1}, {x: x_train, z2: tf.gather(qz2, t)})
inference_z2 = ed.ConjugateInference(
    {z2: qz2}, {x: x_train, z1: tf.gather(qz1, t)})

for _ in range(10000):
  inference_z1.update()
  inference_z2.update()
  increment_t.eval()
\end{lstlisting}

Classic message passing performs exact inference locally in this
manner. Expectation propagation \citep{minka2001expectation} minimizes
$\text{KL}(p || q)$ locally in this manner. Integrated Nested Laplace
Approximation \citep{rue2009approximate} can also be written in this
manner.

\subsubsection{References}\label{references}
