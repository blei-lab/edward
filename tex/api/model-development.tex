\title{Developing Custom Random Variables}

{{navbar}}

\subsubsection{Developing Custom Random Variables}

Oftentimes we'd like to implement our own random variables.
To do so, write a class that inherits
the \texttt{RandomVariable} class in \texttt{edward.models} and
the \texttt{Distribution} class in \texttt{tf.contrib.distributions} (in that
order). A template is provided below.

\begin{lstlisting}[language=Python]
from edward.models import RandomVariable
from tensorflow.contrib.distributions import Distribution

class CustomRandomVariable(RandomVariable, Distribution):
  def __init__(self, *args, **kwargs):
    super(CustomRandomVariable, self).__init__(*args, **kwargs)

  def _log_prob(self, value):
    raise NotImplementedError("log_prob is not implemented")

  def _sample_n(self, n, seed=None):
    raise NotImplementedError("sample_n is not implemented")
\end{lstlisting}

One method that all Edward random variables call during instantiation is
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/distribution.py#L557}{\texttt{_sample_n()}}.
It takes an integer \texttt{n} as input and outputs a tensor of shape
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/distributions/python/ops/distribution.py#L213}{\texttt{(n,) + batch_shape + event_shape}}.
To implement it, you can for example wrap a NumPy/SciPy function
inside the TensorFlow operation \texttt{tf.py_func()}.

For other methods and attributes one can implement, see the API documentation in
TensorFlow's
\href{https://www.tensorflow.org/versions/master/api_docs/python/contrib.distributions/base_classes#Distribution}{\texttt{Distribution} class}.

\subsubsection{Advanced settings}

Sometimes the random variable you'd like to work with already exists
in Edward, but it is missing a particular feature. One hack is to
implement and overwrite the missing method. For example, to implement
your own sampling for \texttt{Poisson}:

\begin{lstlisting}[language=Python]
import edward as ed
from edward.models import Poisson

def _sample_n(self, n=1, seed=None):
  # define Python function which returns samples as a Numpy array
  def np_sample(lam, n):
    return poisson.rvs(mu=lam, size=n, random_state=seed).astype(np.float32)

  # wrap python function as tensorflow op
  val = tf.py_func(np_sample, [self.lam, n], [tf.float32])[0]
  # set shape from unknown shape
  batch_event_shape = self.get_batch_shape().concatenate(self.get_event_shape())
  shape = tf.concat(0, [tf.expand_dims(n, 0),
                        tf.constant(batch_event_shape.as_list(), dtype=tf.int32)])
  val = tf.reshape(val, shape)
  return val

Poisson._sample_n = _sample_n

sess = ed.get_session()
x = Poisson(lam=tf.constant(1.0))
sess.run(x.value())
## 1.0
sess.run(x.value())
## 4.0
\end{lstlisting}

(Note the function \texttt{np_sample} should broadcast correctly if
you'd like to work with non-scalar parameters; it is not correct in
this toy implementation.)

Sometimes the random variable you'd like to work with does not even
admit (easy) sampling, and you're only using it as a likelihood "node" rather
than as some prior to parameters of another random variable.
You can avoid having to implement \texttt{_sample_n} altogether:
after creating \texttt{CustomRandomVariable}, instantiate it with the
\texttt{value} argument:

\begin{lstlisting}[language=Python]
x = CustomRandomVariable(custom_params=params, value=tf.zeros_like(params))
\end{lstlisting}

This fixes the associated value of the random variable to a bunch of
zeros and avoids the \texttt{_sample_n} error that appears otherwise.
Make sure that the value matches the desired shape of the random
variable.
