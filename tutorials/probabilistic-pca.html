<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Probabilistic PCA</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Docstrings. */
  dl.class + dl.class, dl.function + dl.function {
    /* Add border inbetween docstrings. */
    border-top: 1px solid #E1E1E1;
  }
  dt { /* Add spacing for top border. */
    margin-top: 1.0rem;
  }
  dd { /* Remove overall indenting in an entry. */
    margin-left: 0.0rem;
  }
  dl th, dl td { /* Remove extraneous padding and decorations. */
    padding: 0 15px 0 0;
    border: none;
  }
  dt em, dt span.sig-paren { /* Keep style of declarations consistent. */
    font-family: monospace, monospace;
    font-style: normal;
    font-size: 14px !important;
  }
  /* Attribute contents within a docstring. */
  dd blockquote, dl blockquote, dt blockquote { /* Reduce margins. */
    margin-left: 0.0rem;
    margin-top: 0.0rem;
    margin-bottom: 0.0rem;
  }
  dl td p { /* Reduce spacing. */
    margin-bottom: 0.75rem;
  }
  dl td.field-body { /* Add indenting. */
    padding-top: 0.75rem;
    padding-left: 2.0rem;
    display: block;
  }
  dl code { /* Keep code font size consistent with rest of contents. */
    font-size: 90%;
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="/community">Community</a>
    <a class="button u-full-width" href="/contributing">Contributing</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="probabilistic-pca">Probabilistic PCA</h2>
<p>Probabilistic principal components analysis (PCA) is useful for analyzing data via a lower dimensional latent space <span class="citation">(Tipping &amp; Bishop, 1999)</span>. It is often used when there are missing values in the data or for multidimensional scaling.</p>
<p>We demonstrate with an example in Edward. An interactive version with Jupyter notebook is available <a href="http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/docs/notebooks/probabilistic_pca.ipynb">here</a>.</p>
<h3 id="data">Data</h3>
<p>We use simulated data. We’ll talk about the individual variables and what they stand for in the next section. For this example, each data point is 2-dimensional, <span class="math inline">\(\mathbf{x}_n\in\mathbb{R}^2\)</span>.</p>
<pre class="python" language="Python"><code>def build_toy_dataset(N, D, K, sigma=1):
  x_train = np.zeros((D, N))
  w = np.random.normal(0.0, 2.0, size=(D, K))
  z = np.random.normal(0.0, 1.0, size=(K, N))
  mean = np.dot(w, z)
  for d in range(D):
    for n in range(N):
      x_train[d, n] = np.random.normal(mean[d, n], sigma)

  print(&quot;True principal axes:&quot;)
  print(w)
  return x_train

N = 5000  # number of data points
D = 2  # data dimensionality
K = 1  # latent dimensionality

x_train = build_toy_dataset(N, D, K)</code></pre>
<pre><code>## True principal axes:
## [[ 0.25947927]
##  [ 1.80472372]]</code></pre>
<p>We visualize the data set.</p>
<pre class="python" language="Python"><code>plt.scatter(x_train[0, :], x_train[1, :], color=&#39;blue&#39;, alpha=0.1)
plt.axis([-10, 10, -10, 10])
plt.title(&quot;Simulated data set&quot;)
plt.show()</code></pre>
<p><img src="/images/probabilistic-pca-fig0.png" alt="image" width="450" /></p>
<h3 id="model">Model</h3>
<p>Consider a data set <span class="math inline">\(\mathbf{X} = \{\mathbf{x}_n\}\)</span> of <span class="math inline">\(N\)</span> data points, where each data point is <span class="math inline">\(D\)</span>-dimensional, <span class="math inline">\(\mathbf{x}_n \in
\mathbb{R}^D\)</span>. We aim to represent each <span class="math inline">\(\mathbf{x}_n\)</span> under a latent variable <span class="math inline">\(\mathbf{z}_n \in \mathbb{R}^K\)</span> with lower dimension, <span class="math inline">\(K &lt;
D\)</span>. The set of principal axes <span class="math inline">\(\mathbf{W}\)</span> relates the latent variables to the data.</p>
<p>Specifically, we assume that each latent variable is normally distributed, <span class="math display">\[\mathbf{z}_n \sim N(\mathbf{0}, \mathbf{I}).\]</span> The corresponding data point is generated via a projection, <span class="math display">\[\mathbf{x}_n \mid \mathbf{z}_n
\sim N(\mathbf{W}\mathbf{z}_n, \sigma^2\mathbf{I}),\]</span> where the matrix <span class="math inline">\(\mathbf{W}\in\mathbb{R}^{D\times K}\)</span> are known as the principal axes. In probabilistic PCA, we are typically interested in estimating the principal axes <span class="math inline">\(\mathbf{W}\)</span> and the noise term <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Probabilistic PCA generalizes classical PCA. Marginalizing out the the latent variable, the distribution of each data point is <span class="math display">\[\mathbf{x}_n \sim N(\mathbf{0}, \mathbf{W}\mathbf{W}^Y + \sigma^2\mathbf{I}).\]</span> Classical PCA is the specific case of probabilistic PCA when the covariance of the noise becomes infinitesimally small, <span class="math inline">\(\sigma^2 \to
0\)</span>.</p>
<p>We set up our model below. In our analysis, we fix <span class="math inline">\(\sigma=2.0\)</span>, and instead of point estimating <span class="math inline">\(\mathbf{W}\)</span> as a model parameter, we place a prior over it in order to infer a distribution over principal axes.</p>
<pre class="python" language="Python"><code>from edward.models import Normal

w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))
z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))</code></pre>
<h3 id="inference">Inference</h3>
<p>The posterior distribution over the principal axes <span class="math inline">\(\mathbf{W}\)</span> cannot be analytically determined. Below, we set up our inference variables and then run a chosen algorithm to infer <span class="math inline">\(\mathbf{W}\)</span>. Below we use variational inference to minimize the <span class="math inline">\(\text{KL}(q\|p)\)</span> divergence measure.</p>
<pre class="python" language="Python"><code>qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))
qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))

inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})
inference.run(n_iter=500, n_print=100, n_samples=10)</code></pre>
<h3 id="criticism">Criticism</h3>
<p>To check our inferences, we first inspect the model’s learned principal axes.</p>
<pre class="python" language="Python"><code>sess = ed.get_session()
print(&quot;Inferred principal axes:&quot;)
print(sess.run(qw.mean()))</code></pre>
<pre><code>## Inferred principal axes:
## [[-0.24093632]
##  [-1.76468039]]</code></pre>
<p>The model has recovered the true principal axes up to finite data and also up to identifiability (there’s a symmetry in the parameterization).</p>
<p>Another way to criticize the model is to visualize the observed data against data generated from our fitted model.</p>
<pre class="python" language="Python"><code>x_post = ed.copy(x, {w: qw, z: qz})
x_gen = sess.run(x_post)

plt.scatter(x_gen[0, :], x_gen[1, :], color=&#39;red&#39;, alpha=0.1)
plt.axis([-10, 10, -10, 10])
plt.title(&quot;Data generated from model&quot;)
plt.show()</code></pre>
<p><img src="/images/probabilistic-pca-fig1.png" alt="image" width="450" /></p>
<p>The generated data looks close to the true data.</p>
<h3 id="acknowledgments">Acknowledgments</h3>
<p>We thank Mayank Agrawal for writing the initial version of this tutorial.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-tipping1999probabilistic">
<p>Tipping, M. E., &amp; Bishop, C. M. (1999). Probabilistic principal component analysis. <em>Journal of the Royal Statistical Society: Series B (Statistical Methodology)</em>, <em>61</em>(3), 611–622.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
