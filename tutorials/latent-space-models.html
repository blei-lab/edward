<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
<meta content="text/css" http-equiv="Content-Style-Type"/>
<meta content="pandoc" name="generator"/>
<title>Edward – Latent Space Models for Neural Data</title>
<!-- Mobile Specific Metas -->
<meta content="width=device-width, initial-scale=1" name="viewport">
<!-- FONT -->
<link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">
<!-- CSS -->
<link href="/css/normalize.css" rel="stylesheet">
<link href="/css/skeleton.css" rel="stylesheet">
<!-- Dynamically resize logo for mobile -->
<style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  /* Docstrings. */
  dl.class, dl.function { /* Add border to each docstring. */
    border-bottom: 1px solid #E1E1E1;
  }
  dd { /* Remove overall indenting in an entry. */
    margin-left: 0.0rem;
  }
  dl th, dl td { /* Remove extraneous padding and decorations. */
    padding: 0 15px 0 0;
    border: none;
  }
  dt em { /* Keep 'class' consistent in style with rest of def. */
    font-style: normal;
    font-size: 14px !important;
  }
  /* Attribute contents within a docstring. */
  dd blockquote, dl blockquote, dt blockquote { /* Reduce margins. */
    margin-left: 0.0rem;
    margin-top: 0.0rem;
    margin-bottom: 0.0rem;
  }
  dl td p { /* Reduce spacing. */
    margin-bottom: 0.75rem;
  }
  dl td.field-body { /* Add indenting. */
    padding-top: 0.75rem;
    padding-left: 2.0rem;
    display: block;
  }
  dl code { /* Keep code font size consistent with rest of contents. */
    font-size: 90%;
  }
  </style>
<!-- KaTeX -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" rel="stylesheet"/>
<script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>
<!-- highlight.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css" rel="stylesheet">
<!-- Favicon -->
<link href="/icons/apple-touch-icon-57x57.png" rel="apple-touch-icon" sizes="57x57">
<link href="/icons/apple-touch-icon-60x60.png" rel="apple-touch-icon" sizes="60x60">
<link href="/icons/apple-touch-icon-72x72.png" rel="apple-touch-icon" sizes="72x72">
<link href="/icons/apple-touch-icon-76x76.png" rel="apple-touch-icon" sizes="76x76">
<link href="/icons/apple-touch-icon-114x114.png" rel="apple-touch-icon" sizes="114x114">
<link href="/icons/apple-touch-icon-120x120.png" rel="apple-touch-icon" sizes="120x120">
<link href="/icons/apple-touch-icon-144x144.png" rel="apple-touch-icon" sizes="144x144">
<link href="/icons/apple-touch-icon-152x152.png" rel="apple-touch-icon" sizes="152x152">
<link href="/icons/apple-touch-icon-180x180.png" rel="apple-touch-icon" sizes="180x180">
<link href="/icons/favicon-32x32.png" rel="icon" sizes="32x32" type="image/png">
<link href="/icons/android-chrome-192x192.png" rel="icon" sizes="192x192" type="image/png">
<link href="/icons/favicon-96x96.png" rel="icon" sizes="96x96" type="image/png">
<link href="/icons/favicon-16x16.png" rel="icon" sizes="16x16" type="image/png">
<link href="/icons/manifest.json" rel="manifest">
<link color="#5bbad5" href="/icons/safari-pinned-tab.svg" rel="mask-icon">
<link href="/icons/favicon.ico" rel="shortcut icon">
<meta content="#da532c" name="msapplication-TileColor">
<meta content="/icons/mstile-144x144.png" name="msapplication-TileImage">
<meta content="/icons/browserconfig.xml" name="msapplication-config">
<meta content="#ffffff" name="theme-color">
</meta></meta></meta></meta></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></link></meta></head>
<body>
<div class="container">
<div class="row" style="margin-top: 5%">
<div class="three columns">
<h1><a href="/">Edward</a></h1>
<a href="/">
<center>
<img alt="Edward" class="logo-width" src="/images/edward.png"/>
</center>
</a>
<a class="button u-full-width" href="/">Home</a>
<a class="button u-full-width" href="/getting-started">Getting Started</a>
<a class="button u-full-width" href="/tutorials/">Tutorials</a>
<a class="button u-full-width" href="/api/">API</a>
<a class="button u-full-width" href="#">Advanced</a>
<a class="button2 u-full-width" href="/community">Community</a>
<a class="button2 u-full-width" href="/contributing">Contributing</a>
<a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
<div class="row" style="padding-bottom: 5%"> </div>
<a class="button2 u-pull-right" href="https://github.com/blei-lab/edward" style="padding-right:10%">
<span style="vertical-align:middle;">Github</span> 
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
<img alt="Edward on Github" src="/images/github-mark.svg" style="vertical-align:middle;"/>
<!--</object> -->
</a>
<div class="row" style="padding-bottom: 5%"> </div>
</div>
<div class="nine columns">
<h2 id="latent-space-models-for-neural-data">Latent Space Models for Neural Data</h2>
<p>Many scientific fields involve the study of network data, including social networks, networks in statistical physics, biological networks, and information networks <span class="citation">(Goldenberg, Zheng, Fienberg, &amp; Airoldi, 2010; Newman, 2010)</span>.</p>
<p>What we can learn about nodes in a network from their connectivity patterns? We can begin to study this using a latent space model <span class="citation">(Hoff, Raftery, &amp; Handcock, 2002)</span>. Latent space models embed nodes in the network in a latent space, where the likelihood of forming an edge between two nodes depends on their distance in the latent space.</p>
<p>We will analyze network data from neuroscience. An interactive version with Jupyter notebook is available <a href="http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/docs/notebooks/latent_space_models.ipynb">here</a>.</p>
<h3 id="data">Data</h3>
<p>The data comes from <a href="http://www-personal.umich.edu/~mejn/netdata/">Mark Newman’s repository</a>. It is a weighted, directed network representing the neural network of the nematode <a href="https://en.wikipedia.org/wiki/Caenorhabditis_elegans">C. Elegans</a> compiled by <span class="citation">Watts &amp; Strogatz (1998)</span> using experimental data by <span class="citation">White, Southgate, Thomson, &amp; Brenner (1986)</span>.</p>
<p>The neural network consists of around <span class="math inline">\(300\)</span> neurons. Each connection between neurons is associated with a weight (positive integer) capturing the strength of the connection.</p>
<p>First, we load the data.</p>
<pre class="python" language="Python"><code>x_train = np.load('data/celegans_brain.npy')</code></pre>
<h3 id="model">Model</h3>
<p>What can we learn about the neurons from their connectivity patterns? Using a latent space model <span class="citation">(Hoff et al., 2002)</span>, we will learn a latent embedding for each neuron to capture the similarities between them.</p>
<p>Each neuron <span class="math inline">\(n\)</span> is a node in the network and is associated with a latent position <span class="math inline">\(z_n\in\mathbb{R}^K\)</span>. We place a Gaussian prior on each of the latent positions.</p>
<p>The log-odds of an edge between node <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> is proportional to the Euclidean distance between the latent representations of the nodes <span class="math inline">\(|z_i- z_j|\)</span>. Here, we model the weights (<span class="math inline">\(Y_{ij}\)</span>) of the edges with a Poisson likelihood. The rate is the reciprocal of the distance in latent space. The generative process is as follows:</p>
<ol>
<li>For each node <span class="math inline">\(n=1,\ldots,N\)</span>, <span class="math display">\[\begin{aligned}
z_n \sim N(0,I).\end{aligned}\]</span></li>
<li>For each edge <span class="math inline">\((i,j)\in\{1,\ldots,N\}\times\{1,\ldots,N\}\)</span>, <span class="math display">\[\begin{aligned}
Y_{ij} \sim \text{Poisson}\Bigg(\frac{1}{|z_i - z_j|}\Bigg).\end{aligned}\]</span></li>
</ol>
<p>In Edward, we write the model as follows.</p>
<pre class="python" language="Python"><code>from edward.models import Normal, Poisson

N = x_train.shape[0]  # number of data points
K = 3  # latent dimensionality

z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))

# Calculate N x N distance matrix.
# 1. Create a vector, [||z_1||^2, ||z_2||^2, ..., ||z_N||^2], and tile
# it to create N identical rows.
xp = tf.tile(tf.reduce_sum(tf.pow(z, 2), 1, keep_dims=True), [1, N])
# 2. Create a N x N matrix where entry (i, j) is ||z_i||^2 + ||z_j||^2
# - 2 z_i^T z_j.
xp = xp + tf.transpose(xp) - 2 * tf.matmul(z, z, transpose_b=True)
# 3. Invert the pairwise distances and make rate along diagonals to
# be close to zero.
xp = 1.0 / tf.sqrt(xp + tf.diag(tf.zeros(N) + 1e3))

x = Poisson(lam=xp, value=tf.zeros_like(xp))</code></pre>
<p>Note Edward doesn’t currently support sampling for <code>Poisson</code> random variables. For now, we hard-code its associated tensor to be full of 0’s via the <code>value</code> argument. It is required only for instantiating the random variable and is not used in the experiment.</p>
<h3 id="inference">Inference</h3>
<p>Maximum a posteriori (MAP) estimation is simple in Edward. Two lines are required: Instantiating inference and running it.</p>
<pre class="python" language="Python"><code>inference = ed.MAP([z], data={x: x_train})</code></pre>
<p>See this extended tutorial about <a href="/tutorials/map">MAP estimation in Edward</a>.</p>
<p>One could instead run variational inference. This requires specifying a variational model and instantiating <code>KLqp</code>.</p>
<pre class="python" language="Python"><code>qz = Normal(mu=tf.Variable(tf.random_normal([N * K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N * K]))))
inference = ed.KLqp({z: qz}, data={x: x_train})</code></pre>
<p>See this extended tutorial about <a href="/tutorials/variational-inference">variational inference in Edward</a>.</p>
<p>Finally, the following line runs the inference procedure for 2500 iterations.</p>
<pre class="python" language="Python"><code>inference.run(n_iter=2500)</code></pre>
<h3 id="acknowledgments">Acknowledgments</h3>
<p>We thank Maja Rudolph for writing the initial version of this tutorial.</p>
<h3 class="unnumbered" id="references">References</h3>
<div class="references" id="refs">
<div id="ref-goldenberg2010survey">
<p>Goldenberg, A., Zheng, A. X., Fienberg, S. E., &amp; Airoldi, E. M. (2010). A survey of statistical network models. <em>Foundations and Trends in Machine Learning</em>.</p>
</div>
<div id="ref-hoff2002latent">
<p>Hoff, P. D., Raftery, A. E., &amp; Handcock, M. S. (2002). Latent space approaches to social network analysis. <em>Journal of the American Statistical Association</em>, <em>97</em>(460), 1090–1098.</p>
</div>
<div id="ref-newman2010networks">
<p>Newman, M. (2010). <em>Networks: An introduction</em>. Oxford University Press.</p>
</div>
<div id="ref-watts1998collective">
<p>Watts, D. J., &amp; Strogatz, S. H. (1998). Collective dynamics of ‘small-world’networks. <em>Nature</em>, <em>393</em>(6684), 440–442.</p>
</div>
<div id="ref-white1986structure">
<p>White, J. G., Southgate, E., Thomson, J. N., &amp; Brenner, S. (1986). The structure of the nervous system of the nematode caenorhabditis elegans. <em>Philos Trans R Soc Lond B Biol Sci</em>, <em>314</em>(1165), 1–340.</p>
</div>
</div>
</div>
</div>
<div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
