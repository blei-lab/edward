<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Mixture density networks</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/">Home</a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="#">Advanced</a>
    <a class="button2 u-full-width" href="/design-philosophy">Design Philosophy</a>
    <a class="button2 u-full-width" href="/contributing">Contributing</a>
    <a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
    <a class="button2 u-full-width" href="/license">License</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
      <!--</object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="mixture-density-networks">Mixture density networks</h2>
<p>Mixture density networks (MDN) <span class="citation">(Bishop, 1994)</span> are a class of models obtained by combining a conventional neural network with a mixture density model.</p>
<p>We demonstrate how to use MDNs in Edward, leveraging TensorFlow Slim to construct neural networks. The script is available <a href="https://github.com/blei-lab/edward/blob/master/examples/mixture_density_network.py">here</a>.</p>
<h3 id="data">Data</h3>
<p>We use the same toy data from the <a href="http://blog.otoro.net/2015/11/24/mixture-density-networks-with-tensorflow/">David Ha’s blog post</a>, where he explains MDNs. It is an inverse problem where for every input <span class="math inline">\(x_n\)</span> there are multiple outputs <span class="math inline">\(y_n\)</span>.</p>
<pre class="python" language="Python"><code>def build_toy_dataset(N):
  y_data = np.random.uniform(-10.5, 10.5, N).astype(np.float32)
  r_data = np.random.normal(size=N).astype(np.float32)  # random noise
  x_data = np.sin(0.75 * y_data) * 7.0 + y_data * 0.5 + r_data * 1.0
  x_data = x_data.reshape((N, 1))
  return train_test_split(x_data, y_data, random_state=42)

N = 5000  # number of data points
D = 1  # number of features

X_train, X_test, y_train, y_test = build_toy_dataset(N)
print(&quot;Size of features in training data: {}&quot;.format(X_train.shape))
print(&quot;Size of output in training data: {}&quot;.format(y_train.shape))
print(&quot;Size of features in test data: {}&quot;.format(X_test.shape))
print(&quot;Size of output in test data: {}&quot;.format(y_test.shape))

sns.regplot(X_train, y_train, fit_reg=False)</code></pre>
<pre><code>## Size of features in training data: (3750, 1)
## Size of output in training data: (3750,)
## Size of features in test data: (1250, 1)
## Size of output in test data: (1250,)</code></pre>
<p><img src="/images/mdn-fig0.png" alt="image" width="650" /></p>
<p>We define TensorFlow placeholders, which will be used to manually feed batches of data during inference. This is <a href="http://edwardlib.org/api/data">one of many ways</a> to train models with data in Edward.</p>
<pre class="python" language="Python"><code>X_ph = tf.placeholder(tf.float32, [None, D])
y_ph = tf.placeholder(tf.float32, [None])</code></pre>
<h3 id="model">Model</h3>
<p>We use a mixture of 20 normal distributions parameterized by a feedforward network. That is, the membership probabilities and per-component mean and standard deviation are given by the output of a feedforward network. We specify a three-layer network with 15 hidden units for each hidden layer.</p>
<pre class="python" language="Python"><code>def neural_network(X):
  &quot;&quot;&quot;mu, sigma, logits = NN(x; theta)&quot;&quot;&quot;
  # 2 hidden layers with 15 hidden units
  hidden1 = slim.fully_connected(X, 15)
  hidden2 = slim.fully_connected(hidden1, 15)
  mus = slim.fully_connected(hidden2, K, activation_fn=None)
  sigmas = slim.fully_connected(hidden2, K, activation_fn=tf.exp)
  logits = slim.fully_connected(hidden2, K, activation_fn=None)
  return mus, sigmas, logits

K = 20  # number of mixture components

mus, sigmas, logits = neural_network(X_ph)
cat = Categorical(logits=logits)
components = [Normal(mu=mu, sigma=sigma) for mu, sigma
              in zip(tf.unstack(tf.transpose(mus)),
                     tf.unstack(tf.transpose(sigmas)))]
y = Mixture(cat=cat, components=components, value=tf.zeros_like(y_ph))</code></pre>
<p>Note that we use the <code>Mixture</code> random variable. It collapses out the membership assignments for each data point and makes the model differentiable with respect to all its parameters. It takes a <code>Categorical</code> random variable as input—denoting the probability for each cluster assignment—as well as <code>components</code>, which is a list of individual distributions to mix over.</p>
<p>For more background on MDNs, take a look at <a href="http://cbonnett.github.io/MDN.html">Christopher Bonnett’s blog post</a> or at <span class="citation">Bishop (1994)</span>.</p>
<h3 id="inference">Inference</h3>
<p>We use MAP estimation, passing in the model and data set. See this extended tutorial about <a href="/tutorials/map">MAP estimation in Edward</a>.</p>
<pre class="python" language="Python"><code>inference = ed.MAP(data={y: y_ph})</code></pre>
<p>Here, we will manually control the inference and how data is passed into it at each step. Initialize the algorithm and the TensorFlow variables.</p>
<pre class="python" language="Python"><code>inference.initialize(var_list=tf.trainable_variables())

sess = ed.get_session()
tf.global_variables_initializer().run()</code></pre>
<p>Now we train the MDN by calling <code>inference.update()</code>, passing in the data. The quantity <code>inference.loss</code> is the loss function (negative log-likelihood) at that step of inference. We also report the loss function on test data by calling <code>inference.loss</code> and where we feed test data to the TensorFlow placeholders instead of training data. We keep track of the losses under <code>train_loss</code> and <code>test_loss</code>.</p>
<pre class="python" language="Python"><code>n_epoch = 1000
train_loss = np.zeros(n_epoch)
test_loss = np.zeros(n_epoch)
for i in range(n_epoch):
  info_dict = inference.update(feed_dict={X_ph: X_train, y_ph: y_train})
  train_loss[i] = info_dict[&#39;loss&#39;]
  test_loss[i] = sess.run(inference.loss, feed_dict={X_ph: X_test, y_ph: y_test})
  inference.print_progress(info_dict)</code></pre>
<p>After training for a number of iterations, we get out the predictions we are interested in from the model: the predicted mixture weights, cluster means, and cluster standard deviations.</p>
<p>To do this, we fetch their values from session, feeding test data <code>X_test</code> to the placeholder <code>X_ph</code>.</p>
<pre class="python" language="Python"><code>pred_weights, pred_means, pred_std = \
    sess.run([tf.nn.softmax(logits), mus, sigmas], feed_dict={X_ph: X_test})</code></pre>
<p>Let’s plot the log-likelihood of the training and test data as functions of the training epoch. The quantity <code>inference.loss</code> is the total log-likelihood, not the loss per data point. In the plotting routine we get the latter by dividing by the size of the train and test data respectively.</p>
<pre class="python" language="Python"><code>fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(16, 3.5))
plt.plot(np.arange(n_epoch), -test_loss / len(X_test), label=&#39;Test&#39;)
plt.plot(np.arange(n_epoch), -train_loss / len(X_train), label=&#39;Train&#39;)
plt.legend(fontsize=20)
plt.xlabel(&#39;Epoch&#39;, fontsize=15)
plt.ylabel(&#39;Log-likelihood&#39;, fontsize=15)
plt.show()</code></pre>
<p><img src="/images/mdn-fig1.png" alt="image" width="700" /></p>
<p>We see that it converges after 400 iterations.</p>
<h3 id="criticism">Criticism</h3>
<p>Let’s look at how a few individual examples perform. Note that as this is an inverse problem we can’t get the answer correct, but we can hope that the truth lies in area where the model has high probability. This code relies on helper functions available in the full script above.</p>
<p>In this plot the truth is the vertical grey line while the blue line is the prediction of the mixture density network. As you can see, we didn’t do too bad.</p>
<pre class="python" language="Python"><code>obj = [0, 4, 6]
fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(16, 6))

plot_normal_mix(pred_weights[obj][0], pred_means[obj][0], pred_std[obj][0], axes[0], comp=False)
axes[0].axvline(x=y_test[obj][0], color=&#39;black&#39;, alpha=0.5)

plot_normal_mix(pred_weights[obj][2], pred_means[obj][2], pred_std[obj][2], axes[1], comp=False)
axes[1].axvline(x=y_test[obj][2], color=&#39;black&#39;, alpha=0.5)

plot_normal_mix(pred_weights[obj][1], pred_means[obj][1], pred_std[obj][1], axes[2], comp=False)
axes[2].axvline(x=y_test[obj][1], color=&#39;black&#39;, alpha=0.5)</code></pre>
<p><img src="/images/mdn-fig2.png" alt="image" width="700" /></p>
<p>We can check the ensemble by drawing samples of the prediction and plotting the density of those. The MDN has learned what we’d like it to learn.</p>
<pre class="python" language="Python"><code>a = sample_from_mixture(X_test, pred_weights, pred_means, pred_std, amount=len(X_test))
sns.jointplot(a[:,0], a[:,1], kind=&quot;hex&quot;, color=&quot;#4CB391&quot;, ylim=(-10,10), xlim=(-14,14))</code></pre>
<p><img src="/images/mdn-fig3.png" alt="image" width="700" /></p>
<h3 id="acknowledgments">Acknowledgments</h3>
<p>We are grateful to Christopher Bonnett for writing the initial version of this tutorial. More generally, we thank Chris for pushing forward momentum to have Edward tutorials be accessible and easy-to-learn.</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-bishop1994mixture">
<p>Bishop, C. M. (1994). Mixture density networks.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
