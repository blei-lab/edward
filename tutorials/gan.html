<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title>Edward – Generative Adversarial Networks</title>
  <!-- Mobile Specific Metas -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- FONT -->
  <link href="https://fonts.googleapis.com/css?family=Lora:400,400i,700" rel="stylesheet">

  <!-- CSS -->
  <link rel="stylesheet" href="/css/normalize.css">
  <link rel="stylesheet" href="/css/skeleton.css">
  <!-- Dynamically resize logo for mobile -->
  <style type="text/css">
  .logo-width {
    width: 100%;
    box-sizing: border-box;
    margin-bottom: 15%;
  }
  /* Roughly the point when website is single column */
  @media (max-width: 850px) {
    .logo-width {
      width: 50%;
      box-sizing: border-box;
      margin-bottom: 5%;
    }
  }
  </style>

  <!-- KaTeX -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/katex.min.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.6.0/contrib/auto-render.min.js"></script>

  <!-- highlight.js -->
  <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/highlight.min.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.4.0/styles/github.min.css">

  <!-- Favicon -->
  <link rel="apple-touch-icon" sizes="57x57" href="/icons/apple-touch-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/icons/apple-touch-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/icons/apple-touch-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/icons/apple-touch-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/icons/apple-touch-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/icons/apple-touch-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/icons/apple-touch-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/icons/apple-touch-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/icons/apple-touch-icon-180x180.png">
  <link rel="icon" type="image/png" href="/icons/favicon-32x32.png" sizes="32x32">
  <link rel="icon" type="image/png" href="/icons/android-chrome-192x192.png" sizes="192x192">
  <link rel="icon" type="image/png" href="/icons/favicon-96x96.png" sizes="96x96">
  <link rel="icon" type="image/png" href="/icons/favicon-16x16.png" sizes="16x16">
  <link rel="manifest" href="/icons/manifest.json">
  <link rel="mask-icon" href="/icons/safari-pinned-tab.svg" color="#5bbad5">
  <link rel="shortcut icon" href="/icons/favicon.ico">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="msapplication-TileImage" content="/icons/mstile-144x144.png">
  <meta name="msapplication-config" content="/icons/browserconfig.xml">
  <meta name="theme-color" content="#ffffff">
</head>
<body>
<div class="container">
  <div class="row" style="margin-top: 5%">
    <div class="three columns">
    <h1><a href="/">Edward</a></h1>
    <a href="/">
    <center>
      <img src="/images/edward.png" class="logo-width" alt="Edward" />
    </center>
    </a>
    <a class="button u-full-width" href="/">Home</a>
    <a class="button u-full-width" href="/getting-started">Getting Started</a>
    <a class="button u-full-width" href="/tutorials/">Tutorials</a>
    <a class="button u-full-width" href="/api/">API</a>
    <a class="button u-full-width" href="#">Advanced</a>
    <a class="button2 u-full-width" href="/zoo">Probability Zoo</a>
    <a class="button2 u-full-width" href="/contributing">Contributing</a>
    <a class="button2 u-full-width" href="/troubleshooting">Troubleshooting</a>
    <a class="button2 u-full-width" href="/license">License</a>
    <div class="row" style="padding-bottom: 5%"> </div>
    <a class="button2 u-pull-right" style="padding-right:10%"
      href="https://github.com/blei-lab/edward">
      <span style="vertical-align:middle;">Github</span>&nbsp;
      <!--<object data="images/github-mark.svg" type="image/svg+xml"> -->
      <img src="/images/github-mark.svg" style="vertical-align:middle;"
      alt="Edward on Github" />
      <!--</object> -->
    </a>
    <div class="row" style="padding-bottom: 5%"> </div>
    </div>
    <div class="nine columns">

<h2 id="generative-adversarial-networks">Generative Adversarial Networks</h2>
<p>Generative adversarial networks (GANs) are a powerful approach for probabilistic modeling <span class="citation">(I. Goodfellow, 2016; I. Goodfellow et al., 2014)</span>. They posit a deep generative model and they enable fast and accurate inferences.</p>
<p>We demonstrate how to use GANs in Edward. The script is available <a href="https://github.com/blei-lab/edward/blob/master/examples/gan.py">here</a>.</p>
<pre class="python" language="Python"><code>M = 128  # batch size during training
d = 100  # latent dimension

DATA_DIR = &quot;data/mnist&quot;
IMG_DIR = &quot;img&quot;</code></pre>
<h3 id="data">Data</h3>
<p>We use training data from MNIST, which consists of 55,000 <span class="math inline">\(28\times
28\)</span> pixel images <span class="citation">(LeCun, Bottou, Bengio, &amp; Haffner, 1998)</span>. Each image is represented as a flattened vector of 784 elements, and each element is a pixel intensity between 0 and 1.</p>
<p><img src="/images/mnist-train-xs.png" alt="image" width="450" /></p>
<p>The goal is to build and infer a model that can generate high quality images of handwritten digits.</p>
<p>During training we will feed batches of MNIST digits. We instantiate a TensorFlow placeholder with a fixed batch size of <span class="math inline">\(M\)</span> images.</p>
<pre class="python" language="Python"><code>mnist = input_data.read_data_sets(DATA_DIR, one_hot=True)
x_ph = tf.placeholder(tf.float32, [M, 784])</code></pre>
<h3 id="model">Model</h3>
<p>GANs posit generative models using an implicit mechanism. Given some random noise, the data is assumed to be generated by the output of a function of that noise.</p>
<p>Formally, the generative process is <span class="math display">\[\begin{aligned}
\mathbf{z} &amp;\sim p(\mathbf{z}), \\
\mathbf{x} &amp;= G(\mathbf{z}; \theta),\end{aligned}\]</span> where <span class="math inline">\(G(\cdot; \theta)\)</span> is a neural network that takes the latent variable <span class="math inline">\(\mathbf{z}\)</span> as input. The prior <span class="math inline">\(p(\mathbf{z})\)</span> is often interpreted as injected random noise to produce stochasticity in a physical system; it is typically a fixed uniform or normal distribution with some latent dimensionality.</p>
<p>In Edward, we build the model as follows, using TensorFlow Slim to specify the neural network. It defines a 2-layer fully connected neural network and outputs a vector of length <span class="math inline">\(28\times28\)</span> with values in <span class="math inline">\([0,1]\)</span>.</p>
<pre class="python" language="Python"><code>def generative_network(z):
  h1 = slim.fully_connected(z, 128, activation_fn=tf.nn.relu)
  x = slim.fully_connected(h1, 784, activation_fn=tf.sigmoid)
  return x

with tf.variable_scope(&quot;Gen&quot;):
  z = Uniform(a=tf.zeros([M, d]) - 1.0, b=tf.ones([M, d]))
  x = generative_network(z)</code></pre>
<p>We aim to estimate parameters of the generative network such that the model best captures the data. (Note in GANs, we are interested only in parameter estimation and not inference about the latent variables.)</p>
<p>Unfortunately, probability models described above do not admit a tractable likelihood. This poses a problem for most inference algorithms, as they usually require taking the model’s density. Thus we are motivated to use “likelihood-free” algorithms <span class="citation">(Marin, Pudlo, Robert, &amp; Ryder, 2012)</span>, a class of methods which assume one can only sample from the model.</p>
<h3 id="inference">Inference</h3>
<p>A key idea in likelihood-free methods is to learn by comparison (e.g., <span class="citation">Rubin (1984; Gretton, Borgwardt, Rasch, Schölkopf, &amp; Smola, 2012)</span>): by analyzing the discrepancy between samples from the model and samples from the true data distribution, we have information on where the model can be improved in order to generate better samples.</p>
<p>In GANs, a neural network <span class="math inline">\(D(\cdot;\phi)\)</span> makes this comparison, known as the discriminator. <span class="math inline">\(D(\cdot;\phi)\)</span> takes data <span class="math inline">\(\mathbf{x}\)</span> as input (either generations from the model or data points from the data set), and it calculates the probability that <span class="math inline">\(\mathbf{x}\)</span> came from the true data.</p>
<p>In Edward, we use the following discriminative network. It is simply a feedforward network with one ReLU hidden layer. It returns the probability in the logit (unconstrained) scale.</p>
<pre class="python" language="Python"><code>def discriminative_network(x):
  h1 = slim.fully_connected(x, 128, activation_fn=tf.nn.relu)
  logit = slim.fully_connected(h1, 1, activation_fn=None)
  return logit</code></pre>
<p>Let <span class="math inline">\(p^*(\mathbf{x})\)</span> represent the true data distribution. The optimization problem used in GANs is</p>
<p><span class="math display">\[\min_\theta \max_\phi~
\mathbb{E}_{p^*(\mathbf{x})} [ \log D(\mathbf{x}; \phi) ]
+ \mathbb{E}_{p(\mathbf{x}; \theta)} [ \log (1 - D(\mathbf{x}; \phi)) ].\]</span></p>
<p>This optimization problem is bilevel: it requires a min-max solution. In practice, the algorithm proceeds by iterating among these two optimizations, alternating gradient updates. An additional heuristic also modifies the objective function for the generative model in order to avoid saturation of gradients <span class="citation">(I. J. Goodfellow, 2014)</span>.</p>
<p>Many sources of intuition exist behind GAN-style training. One, which is the original motivation, is based on idea that the two neural networks are playing a game. The discriminator tries to best distinguish samples away from the generator. The generator tries to produce samples that are indistinguishable by the discriminator. The goal of training is to reach a Nash equilibrium.</p>
<p>Another source is the idea of casting unsupervised learning as supervised learning <span class="citation">(M. U. Gutmann, Dutta, Kaski, &amp; Corander, 2014; M. Gutmann &amp; Hyvärinen, 2010)</span>. This allows one to leverage the power of classification—a problem that in recent years is (relatively speaking) very easy.</p>
<p>A third comes from classical statistics, where the discriminator is interpreted as a proxy of the density ratio between the true data distribution and the model <span class="citation">(Mohamed &amp; Lakshminarayanan, 2016; Sugiyama, Suzuki, &amp; Kanamori, 2012)</span>. By augmenting an original problem that may require the model’s density with a discriminator (such as maximum likelihood), one can recover the original problem when the discriminator is optimal. Furthermore, this approximation is very fast, and it justifies GANs from the perspective of approximate inference.</p>
<p>In Edward, the GAN algorithm (<code>GANInference</code>) simply takes the implicit density model on <code>x</code> as input, binded to its realizations <code>x_ph</code>. In addition, a parameterized function <code>discriminator</code> is provided to distinguish their samples.</p>
<pre class="python" language="Python"><code>inference = ed.GANInference(
    data={x: x_ph}, discriminator=discriminative_network)</code></pre>
<p>We’ll use ADAM as optimizers for both the generator and discriminator. We’ll run the algorithm for 15,000 iterations and print progress every 1,000 iterations.</p>
<pre class="python" language="Python"><code>optimizer = tf.train.AdamOptimizer()
optimizer_d = tf.train.AdamOptimizer()

inference.initialize(
    optimizer=optimizer, optimizer_d=optimizer,
    n_iter=15000, n_print=1000)</code></pre>
<p>We now form the main loop which trains the GAN. At each iteration, it takes a minibatch and updates the parameters according to the algorithm. At every 1000 iterations, it will print progress. It also saves a figure of generated samples from the model.</p>
<pre class="python" language="Python"><code>sess = ed.get_session()
tf.global_variables_initializer().run()

idx = np.random.randint(M, size=16)
i = 0
for t in range(inference.n_iter):
  if t % inference.n_print == 0:
    samples = sess.run(x)
    samples = samples[idx, ]

    fig = plot(samples)
    plt.savefig(os.path.join(IMG_DIR, &#39;{}.png&#39;).format(
        str(i).zfill(3)), bbox_inches=&#39;tight&#39;)
    plt.close(fig)
    i += 1

  x_batch, _ = mnist.train.next_batch(M)
  info_dict = inference.update(feed_dict={x_ph: x_batch})
  inference.print_progress(info_dict)</code></pre>
<p>Examining convergence of the GAN objective can be meaningless in practice. The algorithm is usually run until some other criterion is satisfied, such as if the samples look visually okay, or if the GAN can capture meaningful parts of the data.</p>
<h3 id="criticism">Criticism</h3>
<p>Evaluation of GANs remains an open problem—both in criticizing their fit to data and in assessing convergence. Recent advances have considered alternative objectives and heuristics to stabilize training (see also Soumith Chintala’s <a href="https://github.com/soumith/ganhacks">GAN hacks repo</a>).</p>
<p>As one approach to criticize the model, we simply look at generated images during training. Below we show generations after 14,000 iterations (that is, 14,000 gradient updates of both the generator and the discriminator).</p>
<p><img src="/images/mnist_gan.png" alt="image" width="500" /></p>
<p>The images are meaningful albeit a little blurry. Suggestions for further improvements would be to tune the hyperparameters in the optimization, to improve the capacity of the discriminative and generative networks, and to leverage more prior information (such as convolutional architectures).</p>
<h3 id="references" class="unnumbered">References</h3>
<div id="refs" class="references">
<div id="ref-goodfellow2016nips">
<p>Goodfellow, I. (2016). NIPS 2016 Tutorial: Generative Adversarial Networks. <em>ArXiv Preprint ArXiv:1611.06953</em>.</p>
</div>
<div id="ref-goodfellow2014on">
<p>Goodfellow, I. J. (2014). On distinguishability criteria for estimating generative models. In <em>ICLR workshop</em>.</p>
</div>
<div id="ref-goodfellow2014generative">
<p>Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., … Bengio, Y. (2014). Generative adversarial nets. In <em>Neural information processing systems</em>.</p>
</div>
<div id="ref-gretton2012kernel">
<p>Gretton, A., Borgwardt, K. M., Rasch, M. J., Schölkopf, B., &amp; Smola, A. (2012). A kernel two-sample test. <em>The Journal of Machine Learning Research</em>, <em>13</em>, 723–773.</p>
</div>
<div id="ref-gutmann2014statistical">
<p>Gutmann, M. U., Dutta, R., Kaski, S., &amp; Corander, J. (2014). Statistical Inference of Intractable Generative Models via Classification. <em>ArXiv Preprint ArXiv:1407.4981</em>.</p>
</div>
<div id="ref-gutmann2010noise">
<p>Gutmann, M., &amp; Hyvärinen, A. (2010). Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In <em>Artificial intelligence and statistics</em>.</p>
</div>
<div id="ref-lecun1998gradient">
<p>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE</em>, <em>86</em>(11), 2278–2324.</p>
</div>
<div id="ref-marin2012approximate">
<p>Marin, J.-M., Pudlo, P., Robert, C. P., &amp; Ryder, R. J. (2012). Approximate Bayesian computational methods. <em>Statistics and Computing</em>, <em>22</em>(6), 1167–1180.</p>
</div>
<div id="ref-mohamed2016learning">
<p>Mohamed, S., &amp; Lakshminarayanan, B. (2016). Learning in Implicit Generative Models. <em>ArXiv Preprint ArXiv:1610.03483</em>.</p>
</div>
<div id="ref-rubin1984bayesianly">
<p>Rubin, D. B. (1984). Bayesianly justifiable and relevant frequency calculations for the applied statistician. <em>The Annals of Statistics</em>, <em>12</em>(4), 1151–1172.</p>
</div>
<div id="ref-sugiyama2012density">
<p>Sugiyama, M., Suzuki, T., &amp; Kanamori, T. (2012). Density-ratio matching under the Bregman divergence: a unified framework of density-ratio estimation. <em>Annals of the Institute of Statistical …</em>.</p>
</div>
</div>
    </div>
  </div>
  <div class="row" style="padding-bottom: 25%"> </div>
</div>
<script>
  hljs.initHighlightingOnLoad();
  renderMathInElement(document.body);
</script>
</body>
</html>
