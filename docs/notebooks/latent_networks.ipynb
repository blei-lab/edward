{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent space models for neural data\n",
    "\n",
    "Many scientific fields involve the study of network data, including social networks, networks in statistical physics, biological networks, and information networks (Goldenberg, Zheng, Fienberg, & Airoldi, 2010; Newman, 2010).\n",
    "\n",
    "What we can learn about nodes in a network from their connectivity patterns? We can begin to study this using a latent space model (Hoff, Raftery, & Handcock, 2002). Latent space models embed nodes in the network in a latent space, where the likelihood of forming an edge between two nodes depends on their distance in the latent space.\n",
    "\n",
    "We will analyze network data from neuroscience. The full script can be found [here](https://github.com/blei-lab/edward/blob/master/examples/latent_space_model.py).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal, Poisson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "\n",
    "The data comes from [Mark Newman’s repository](http://www-personal.umich.edu/~mejn/netdata/). It is a weighted, directed network representing the neural network of the nematode [C. Elegans](https://en.wikipedia.org/wiki/Caenorhabditis_elegans) compiled by Watts & Strogatz (1998) using experimental data by White, Southgate, Thomson, & Brenner (1986).\n",
    "\n",
    "The neural network consists of around $300$ neurons. Each connection between neurons is associated with a weight (positive integer) capturing the strength of the connection.\n",
    "\n",
    "We load the data with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(42)\n",
    "\n",
    "# DATA\n",
    "x_train = np.load('data/celegans_brain.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What can we learn about the neurons from their connectivity patterns? Using a latent space model (Hoff et al., 2002), we will learn a latent embedding for each neuron to capture the similarities between them.\n",
    "\n",
    "Each neuron $n$ is a node in the network and is associated with a latent position $z_n \\in \\mathbb{R}^{K}$.\n",
    "We place a Gaussian prior on each of the latent vectors. The log-odds of an edge between node $i$ and $j$ is proportional to the Euclidean distance between the latent representations of the nodes $|z_i- z_j|$. Here, we model the weights $Y_{ij}$ of the edges with a Poisson likelihood. The rate is the reciprocal of the distance in latent \n",
    "\n",
    "<p>For each node $n = 1, . . ., \\mathbf{N}, $<span class=\"math display\">\\\\begin{aligned}\n",
    "z_n \\sim N(0,I).\\end{aligned}</span> \n",
    "\n",
    "\n",
    "For each edge $(i, j) \\in \\{1, . . ., \\mathbf{N}\\} X \\{1, . . ., \\mathbf{N}\\}$,  <span class=\"math display\">\n",
    "\\begin{aligned}\n",
    "Y_{ij} \\sim \\text{Poisson}\\Bigg(\\frac{1}{|z_i - z_j|}\\Bigg).\\end{aligned}</span></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = x_train.shape[0]  # number of data points\n",
    "K = 3  # latent dimensionality\n",
    "\n",
    "z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))\n",
    "\n",
    "# Calculate N x N distance matrix.\n",
    "# 1. Create a vector, [||z_1||^2, ||z_2||^2, ..., ||z_N||^2], and tile\n",
    "# it to create N identical rows.\n",
    "xp = tf.tile(tf.reduce_sum(tf.pow(z, 2), 1, keep_dims=True), [1, N])\n",
    "# 2. Create a N x N matrix where entry (i, j) is ||z_i||^2 + ||z_j||^2\n",
    "# - 2 z_i^T z_j.\n",
    "xp = xp + tf.transpose(xp) - 2 * tf.matmul(z, z, transpose_b=True)\n",
    "# 3. Invert the pairwise distances and make rate along diagonals to\n",
    "# be close to zero.\n",
    "xp = 1.0 / tf.sqrt(xp + tf.diag(tf.zeros(N) + 1e3))\n",
    "\n",
    "x = Poisson(lam=xp, value=tf.zeros_like(xp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note Edward doesn’t currently support sampling for `Poisson` random variables. For now, we hard-code its associated tensor to be full of 0’s via the `value` argument. It is required only for instantiating the random variable and is not used in the experiment.\n",
    "\n",
    "### Inference\n",
    "\n",
    "Maximum a posteriori (MAP) estimation is simple in Edward. Two lines are required: Instantiating inference and running it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inference = ed.MAP([z], data={x: x_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this extended tutorial about [MAP estimation in Edward](http://edwardlib.org/tutorials/map).\n",
    "\n",
    "One could instead run variational inference. This requires specifying a variational model and instantiating `KLqp`.\n",
    "The code is commented out here but you can give it a shot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qz = Normal(mu=tf.Variable(tf.random_normal([N * K])),\n",
    "#             sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N * K]))))\n",
    "# inference = ed.KLqp({z: qz}, data={x: x_train})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following line runs the inference procedure for 2500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    1 [  0%]: Loss = 68587.555\n",
      "Iteration  250 [ 10%]: Loss = 39034.852\n",
      "Iteration  500 [ 20%]: Loss = 37379.715\n",
      "Iteration  750 [ 30%]: Loss = 36396.875\n",
      "Iteration 1000 [ 40%]: Loss = 36234.805\n",
      "Iteration 1250 [ 50%]: Loss = 36106.988\n",
      "Iteration 1500 [ 60%]: Loss = 35916.383\n",
      "Iteration 1750 [ 70%]: Loss = 35826.727\n",
      "Iteration 2000 [ 80%]: Loss = 35762.750\n",
      "Iteration 2250 [ 90%]: Loss = 35720.777\n",
      "Iteration 2500 [100%]: Loss = 35689.887\n"
     ]
    }
   ],
   "source": [
    "inference.run(n_iter=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See this extended tutorial about [variational inference](http://edwardlib.org/tutorials/variational-inference) in Edward.\n",
    "\n",
    "### References\n",
    "\n",
    "Goldenberg, A., Zheng, A. X., Fienberg, S. E., & Airoldi, E. M. (2010). A survey of statistical network models. Foundations and Trends in Machine Learning.\n",
    "\n",
    "Hoff, P. D., Raftery, A. E., & Handcock, M. S. (2002). Latent space approaches to social network analysis. Journal of the American Statistical Association, 97(460), 1090–1098.\n",
    "\n",
    "Newman, M. (2010). Networks: An introduction. Oxford University Press.\n",
    "\n",
    "Watts, D. J., & Strogatz, S. H. (1998). Collective dynamics of ‘small-world’networks. Nature, 393(6684), 440–442.\n",
    "\n",
    "White, J. G., Southgate, E., Thomson, J. N., & Brenner, S. (1986). The structure of the nervous system of the nematode caenorhabditis elegans. Philos Trans R Soc Lond B Biol Sci, 314(1165), 1–340.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
