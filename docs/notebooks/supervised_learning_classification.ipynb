{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised learning (Classification)\n",
    "\n",
    "In supervised learning, the task is to infer hidden structure from labeled data, comprised of training examples { $(x_n, y_n)$ }. Classification means the output $y$ takes discrete values.\n",
    "\n",
    "We demonstrate how to do this in Edward with an example. The script is available [here](https://github.com/blei-lab/edward/blob/master/examples/tf_gp_classification.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Bernoulli, MultivariateNormalFull, Normal\n",
    "from edward.util import multivariate_rbf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Use 25 data points from the crabs data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ed.set_seed(42)\n",
    "# DATA\n",
    "df = np.loadtxt('data/crabs_train.txt', dtype='float32', delimiter=',')\n",
    "df[df[:, 0] == -1, 0] = 0  # replace -1 label with 0 label\n",
    "N = 25  # number of data points\n",
    "D = df.shape[1] - 1  # number of features\n",
    "subset = np.random.choice(df.shape[0], N, replace=False)\n",
    "X_train = df[subset, 1:]\n",
    "y_train = df[subset, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Posit the model as Gaussian process classification. For more details on the model, see the [Gaussian process classification tutorial](http://edwardlib.org/tutorials/gp-classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kernel(x):\n",
    "  mat = []\n",
    "  for i in range(N):\n",
    "    mat += [[]]\n",
    "    xi = x[i, :]\n",
    "    for j in range(N):\n",
    "      if j == i:\n",
    "        mat[i] += [multivariate_rbf(xi, xi)]\n",
    "      else:\n",
    "        xj = x[j, :]\n",
    "        mat[i] += [multivariate_rbf(xi, xj)]\n",
    "\n",
    "    mat[i] = tf.pack(mat[i])\n",
    "\n",
    "  return tf.pack(mat)\n",
    "\n",
    "# MODEL\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "f = MultivariateNormalFull(mu=tf.zeros(N), sigma=kernel(X))\n",
    "y = Bernoulli(logits=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Perform variational inference. Define the variational model to be a fully factorized normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INFERENCE\n",
    "qf = Normal(mu=tf.Variable(tf.random_normal([N])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the inference for 500 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration   1 [  0%]: Loss = 96.818\n",
      "Iteration  50 [ 10%]: Loss = 19.040\n",
      "Iteration 100 [ 20%]: Loss = 24.368\n",
      "Iteration 150 [ 30%]: Loss = 19.412\n",
      "Iteration 200 [ 40%]: Loss = 20.554\n",
      "Iteration 250 [ 50%]: Loss = 20.292\n",
      "Iteration 300 [ 60%]: Loss = 24.861\n",
      "Iteration 350 [ 70%]: Loss = 19.802\n",
      "Iteration 400 [ 80%]: Loss = 19.789\n",
      "Iteration 450 [ 90%]: Loss = 23.933\n",
      "Iteration 500 [100%]: Loss = 22.998\n"
     ]
    }
   ],
   "source": [
    "data = {X: X_train, y: y_train}\n",
    "inference = ed.KLqp({f: qf}, data)\n",
    "inference.run(n_iter=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case `KLqp` defaults to minimizing the $KL(q\\Vert p)$ divergence measure using the reparameterization gradient. For more details on inference, see the $KL(q\\Vert p)$ [tutorial](http://edwardlib.org/tutorials/klqp). \n",
    "(This example happens to be slow because evaluating and inverting full covariances in Gaussian processes happens to be slow.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
