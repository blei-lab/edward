{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probabilistic PCA\n",
    "\n",
    "Probabilistic principal components analysis (probabilistic PCA) is\n",
    "useful for analyzing data via a lower dimensional latent space\n",
    "(Tipping & Bishop, 1999). It is often\n",
    "used when there are missing values in the data or for multidimensional\n",
    "scaling.\n",
    "\n",
    "We demonstrate with an example in Edward. A webpage version is available \n",
    "[here](http://edwardlib.org/tutorials/probabilistic-pca),\n",
    "or as a script available at\n",
    "[`examples/probabilistic_pca.py`](https://github.com/blei-lab/edward/blob/master/examples/probabilistic_pca.py)\n",
    "in the Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import edward as ed\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We use simulated data. We'll talk about the individual variables and\n",
    "what they stand for in the next section. For this example, each data\n",
    "point is 2-dimensional, $\\mathbf{x}_n\\in\\mathbb{R}^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(N, D, K, sigma=1):\n",
    "  x_train = np.zeros((D, N))\n",
    "  w = np.random.normal(0.0, 2.0, size=(D, K))\n",
    "  z = np.random.normal(0.0, 1.0, size=(K, N))\n",
    "  mean = np.dot(w, z)\n",
    "  for d in range(D):\n",
    "    for n in range(N):\n",
    "      x_train[d, n] = np.random.normal(mean[d, n], sigma)\n",
    "\n",
    "  print(\"True principal axes:\")\n",
    "  print(w)\n",
    "  return x_train\n",
    "\n",
    "ed.set_seed(142)\n",
    "\n",
    "N = 5000  # number of data points\n",
    "D = 2  # data dimensionality\n",
    "K = 1  # latent dimensionality\n",
    "\n",
    "# DATA\n",
    "\n",
    "x_train = build_toy_dataset(N, D, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates a dataset that looks like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Consider a set of $N$ data points, $X = \\{\\mathbf{x}_n\\}$, where each\n",
    "data point is $d$-dimensional, $\\mathbf{x}_n \\in \\mathbb{R}^D$.\n",
    "\n",
    "While our data operates in $D$ dimensions, we will use latent variables\n",
    "$\\mathbf{z}_n \\in \\mathbb{R}^K$ for each data point $\\mathbf{x}_n$\n",
    "with lower dimension, $K < D$. The set of principal axes\n",
    "$\\mathbf{W}$ relates our latent variables to our actual data.\n",
    "\n",
    "Specifically, we have that\n",
    "\\begin{equation*}\n",
    "\\mathbf{z} \\sim N(0, \\mathbf{I}),\n",
    "\\end{equation*}\n",
    "and that\n",
    "\\begin{equation*}\n",
    "\\mathbf{x} \\mid \\mathbf{z} \\sim N(\\mathbf{Wz} + \\mu, \\sigma^2\\mathbf{I}),\n",
    "\\end{equation*}\n",
    "where $\\mathbf{\\epsilon} \\sim N(0, \\sigma^2\\mathbf{I})$.\n",
    "\n",
    "Because of the dependence on $\\mathbf{Wz}$, we don't want to solely\n",
    "want to infer the original principal axes (as would be done in the\n",
    "vanilla PCA model). Rather, we aim to model the principal axes with\n",
    "respect to the latent variables.\n",
    "\n",
    "For our marginal distribution, we get that\n",
    "\\begin{equation*}\n",
    "x \\sim N(0, \\mathbf{W}\\mathbf{W}^Y + \\sigma^2\\mathbf{I}).\n",
    "\\end{equation*}\n",
    "\n",
    "Note here that regular PCA is simply the specific case of\n",
    "Probabilistic PCA, as $\\sigma^2 \\to 0$.\n",
    "\n",
    "We set up our model below, forming creating a prior distribution over\n",
    "our variables of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODEL\n",
    "\n",
    "w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))\n",
    "z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))\n",
    "x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Since the principal axes $\\mathbf{W}$ cannot be analytically\n",
    "determined, we must use some inference method. Below, we set up our\n",
    "inference variables and then run a chosen algorithm. For this\n",
    "example, we minimize the $\\text{KL}(q\\|p)$ divergence measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))\n",
    "qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))\n",
    "\n",
    "inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})\n",
    "inference.run(n_iter=500, n_print=100, n_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criticism\n",
    "\n",
    "One way to criticize the model is to visually compare our actual data\n",
    "to data produced by our inferred values. The blue dots represent the\n",
    "original data, while the red is the inferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
