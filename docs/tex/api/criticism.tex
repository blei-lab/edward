\title{Criticism}

{{navbar}}

\subsubsection{Criticism}

We can never validate whether a model is true. In practice, ``all
models are wrong'' \citep{box1976science}. However, we can try to
uncover where the model goes wrong. Model criticism helps justify the
model as an approximation or point to good directions for revising the
model.
For background, see the criticism \href{/tutorials/criticism}{tutorial}.

Edward explores model criticism using
\begin{itemize}
  \item point evaluations, such as mean squared error or
  classification accuracy;
  \item posterior predictive checks, for making probabilistic
  assessments of the model fit using discrepancy functions.
\end{itemize}

% \subsubsection{Developing new criticism techniques}

% Criticism is defined simply with utility functions. They take random
% variables as input and output NumPy arrays.
% Criticism techniques are simply functions which take as input data,
% the probability model and variational model (binded through a latent
% variable dictionary), and any additional inputs.

% \begin{lstlisting}[language=Python]
% def criticize(data, latent_vars, ...)
%   ...
% \end{lstlisting}

% Developing new criticism techniques is easy.  They can be derived from
% the current techniques or built as a standalone function.

\begin{center}\rule{3in}{0.4pt}\end{center}

{%sphinx

.. automodule:: edward.criticisms
   :members: evaluate,
             ppc

%}

\subsubsection{References}\label{references}
