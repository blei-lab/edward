\title{Point-based evaluations}

\subsection{Point-based evaluations}

A point-based evaluation is a scalar-valued metric for assessing
trained models \citep{gneiting2007strictly}.
For example, we can assess models for classification
by predicting the label for each observation in the data and comparing
it to their true labels. Edward implements a variety of metrics, such
as classification error and mean absolute error.

Formally, point prediction in probabilistic models is given by
taking the mean of the posterior predictive distribution,
\begin{align*}
  p(x_\text{new} \mid x)
  &=
  \int
  p(x_\text{new} \mid z)
  p(z \mid x)
  \text{d} z.
\end{align*}
The model's posterior predictive can be used to generate new data
given past observations and can also make predictions on new data
given past observations.
It is formed by calculating the likelihood of the new data, averaged
over every set of latent variables according to the posterior
distribution.

\subsubsection{Implementation}

To evaluate inferred models, we first form the posterior
predictive distribution. A helpful utility function for this is
\texttt{copy}. For example,
assume the model defines a likelihood \texttt{x} connected to a prior
\texttt{z}. The posterior predictive distribution is
\begin{lstlisting}[language=Python]
x_post = ed.copy(x, {z: qz})
\end{lstlisting}
Here, we copy the likelihood node \texttt{x} in the graph and replace dependence
on the prior \texttt{z} with dependence on the inferred posterior \texttt{qz}.

The \texttt{ed.evaluate()} method takes as input a set of metrics to
evaluate, and a data dictionary. As with inference, the data dictionary binds the
observed random variables in the model to realizations: in this case,
it is the posterior predictive random variable of outputs \texttt{y_post} to
\texttt{y_train} and a placeholder for inputs \texttt{x} to
\texttt{x_train}.
\begin{lstlisting}[language=Python]
ed.evaluate('categorical_accuracy', data={y_post: y_train, x: x_train})
ed.evaluate('mean_absolute_error', data={y_post: y_train, x: x_train})
\end{lstlisting}
The \texttt{data} can be data held-out from training time, making it
easy to implement cross-validated techniques.

Point-based evaluation applies generally to any setting, including
unsupervised tasks. For example, we can evaluate the likelihood of
observing the data.
\begin{lstlisting}[language=Python]
ed.evaluate('log_likelihood', data={x_post: x_train})
\end{lstlisting}

Point-based evaluations are formally known as scoring rules
in decision theory. Scoring rules are useful for model comparison, model
selection, and model averaging.

See the \href{/api/criticism}{criticism API} for further details.
An example of point-based evaluation is in the
\href{/tutorial/supervised-regression}{supervised learning
(regression)} tutorial.

\subsubsection{Implementation (model wrappers)}

For model wrappers, Edward implements point-based evaluations through
the \texttt{predict()} method in the class object. It predicts
the label given samples from the posterior $p(z \mid x)$, i.e., it is
the mean of $p(x_\text{new} \mid z)$ for a posterior sample.
\begin{lstlisting}[language=Python]
class BayesianLinearRegression:
  ...
  def predict(self, xs, zs):
    """Return a prediction for each data point, via the likelihood's
    mean."""
    x = xs['x']
    w, b = zs['w'], zs['b']
    return ed.dot(x, w) + b

model = BayesianLinearRegression()
\end{lstlisting}
Examples of evaluation on model wrappers are
\begin{lstlisting}[language=Python]
ed.evaluate('categorical_accuracy', data={'y': y_train, 'x': x_train},
            latent_vars={'z': qz}, model_wrapper=model)
ed.evaluate('mean_absolute_error', data={'y': y_train, 'x': x_train},
            latent_vars={'z': qz}, model_wrapper=model)
\end{lstlisting}

\subsubsection{References}\label{references}
