\title{Supervised learning (Regression)}

\subsection{Supervised learning (Regression)}

In supervised learning, the task is to infer hidden structure from
labeled data, comprised of training examples $\{(x_n, y_n)\}$.
Regression (typically) means the output $y$ takes continuous values.

We demonstrate how to do this in Edward with an example.
The script is available
\href{https://github.com/blei-lab/edward/blob/master/examples/bayesian_linear_regression_10d.py}
{here}.


\subsubsection{Data}

Simulate training and test sets of $40$ data points. They comprise of
pairs of inputs $\mathbf{x}_n\in\mathbb{R}^{10}$ and outputs
$y_n\in\mathbb{R}$. They have a linear dependence with normally
distributed noise.

\begin{lstlisting}[language=Python]
def build_toy_dataset(N, coeff=np.random.randn(10), noise_std=0.1):
  n_dim = len(coeff)
  x = np.random.randn(N, n_dim).astype(np.float32)
  y = np.dot(x, coeff) + norm.rvs(0, noise_std, size=N)
  return x, y

N = 40  # number of data points
D = 10  # number of features

coeff = np.random.randn(D)
X_train, y_train = build_toy_dataset(N, coeff)
X_test, y_test = build_toy_dataset(N, coeff)
\end{lstlisting}


\subsubsection{Model}

Posit the model as Bayesian linear regression. For more details on the
model, see the
\href{/tutorials/bayesian-linear-regression}
{Bayesian linear regression tutorial}.

\begin{lstlisting}[language=Python]
X = tf.placeholder(tf.float32, [N, D])
w = Normal(mu=tf.zeros(D), sigma=tf.ones(D))
b = Normal(mu=tf.zeros(1), sigma=tf.ones(1))
y = Normal(mu=ed.dot(X, w) + b, sigma=tf.ones(N))
\end{lstlisting}


\subsubsection{Inference}

Perform variational inference.
Define the variational model to be a fully factorized normal across
the weights.
\begin{lstlisting}[language=Python]
qw = Normal(mu=tf.Variable(tf.random_normal([D])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D]))))
qb = Normal(mu=tf.Variable(tf.random_normal([1])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))
\end{lstlisting}

Run variational inference for 1000 iterations.
\begin{lstlisting}[language=Python]
data = {X: X_train, y: y_train}
inference = ed.KLqp({w: qw, b: qb}, data)
inference.run()
\end{lstlisting}
In this case \texttt{KLqp} defaults to minimizing the
$\text{KL}(q\|p)$ divergence measure using the reparameterization
gradient.
For more details on inference, see the \href{/tutorials/klqp}{$\text{KL}(q\|p)$ tutorial}.


\subsubsection{Criticism}

Use point-based evaluation, and calculate the mean squared
error for predictions on test data.

We do this first by forming the posterior predictive distribution.
\begin{lstlisting}[language=Python]
y_post = Normal(mu=ed.dot(X, qw.mean()) + qb.mean(), sigma=tf.ones(N))
\end{lstlisting}

Evaluate predictions from the posterior predictive.
\begin{lstlisting}[language=Python]
print(ed.evaluate('mean_squared_error', data={X: X_test, y_post: y_test}))
## 0.0399784
\end{lstlisting}

The trained model makes predictions with low mean squared error
(relative to the magnitude of the output).

For more details on criticism, see the
\href{/tutorials/point-evaluation}{point-based
evaluation tutorial}.
