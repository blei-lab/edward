\newcommand{\R}{ \mbox{${\Bbb R}$}}


\title{Probabilistic PCA}

\subsection{Probabilistic PCA}

Probabilistic PCA is useful when dealing with data that is dependent on both principal axes and latent variables \citep{Tipping99probabilisticprincipal}. It is often used when there are missing values in the data.

We'll run through a quick tutorial below. The full script can be accessed
\href{https://github.com/blei-lab/edward/blob/master/examples/probabilistic_pca.py}
{here}.

Consider a dataset $X = \{(\mathbf{x}_1, \mathbf{x}_2,\ldots , \mathbf{x}_n)\}$ where $\mathbf{x}_i \in \Bbb{R}^D$.

While our data operates in dimension $D$, we will have latent variables $\mathbf{z}_i \in \Bbb{R}^K$ for every $\mathbf{x}_i$ such that $K < D$. The set of principal axes $\mathbf{W}$ relates our latent variables to our actual data.

Thus, we have that $$\mathbf{z} \sim N(0, \mathbf{I})$$ and that $$\mathbf{x} \vert \mathbf{z} \sim N(\mathbf{Wz} + \mu, \sigma^2\mathbf{I})$$ where $\mathbf{\epsilon} \sim N(0, \sigma^2\mathbf{I})$.

Because of the dependence on $\mathbf{Wz}$, we don't want to solely want to infer the original principal axes (as would be done in the vanilla PCA model). Rather, we aim to model the principal axes with respect to the latent variables.

We set up our model below.

\begin{lstlisting}[language=Python]
w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))
z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))

\end{lstlisting}

Since $\mathbf{W}$ cannot be analytically determined, we must use some approximation method. Below, we set up our inference variables and then run the approximation algorithm. For this example, our method is to minimize the $\text{KL}(q\|p)$ divergence measure.

\begin{lstlisting}[language=Python]
qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))
qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))

inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})
\end{lstlisting}

\subsubsection{References}\label{references}
