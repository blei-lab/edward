\title{Probabilistic PCA}

\subsection{Probabilistic PCA}

Probabilistic principal components analysis (PCA) is
useful for analyzing data via a lower dimensional latent space
\citep{tipping1999probabilistic}. It is often
used when there are missing values in the data or for multidimensional
scaling.

We demonstrate with an example in Edward.
An interactive version with Jupyter notebook is available
\href{http://nbviewer.jupyter.org/github/blei-lab/edward/blob/master/docs/notebooks/probabilistic_pca.ipynb}{here},
or as a script available at
\href{https://github.com/blei-lab/edward/blob/master/examples/probabilistic_pca.py}
{\texttt{examples/probabilistic_pca.py}} in the Github repository.

\subsubsection{Data}

We use simulated data. We'll talk about the individual variables and
what they stand for in the next section. For this example, each data
point is 2-dimensional, $\mathbf{x}_n\in\mathbb{R}^2$.

\begin{lstlisting}[language=Python]
def build_toy_dataset(N, D, K, sigma=1):
  x_train = np.zeros((D, N))
  w = np.random.normal(0.0, 2.0, size=(D, K))
  z = np.random.normal(0.0, 1.0, size=(K, N))
  mean = np.dot(w, z)
  for d in range(D):
    for n in range(N):
      x_train[d, n] = np.random.normal(mean[d, n], sigma)

  print("True principal axes:")
  print(w)
  return x_train

N = 5000  # number of data points
D = 2  # data dimensionality
K = 1  # latent dimensionality

x_train = build_toy_dataset(N, D, K)
\end{lstlisting}

\begin{lstlisting}
## True principal axes:
## [[ 0.25947927]
##  [ 1.80472372]]
\end{lstlisting}

We visualize the data set.

\begin{lstlisting}[language=Python]
plt.scatter(x_train[0, :], x_train[1, :], color='blue', alpha=0.1)
plt.axis([-10, 10, -10, 10])
plt.title("Simulated data set")
plt.show()
\end{lstlisting}

\includegraphics[width=450px]{/images/probabilistic-pca-fig0.png}

\subsubsection{Model}

Consider a set of $N$ data points, $X = \{\mathbf{x}_n\}$, where each
data point is $d$-dimensional, $\mathbf{x}_n \in \mathbb{R}^D$.

While our data operates in $D$ dimensions, we will use latent variables
$\mathbf{z}_n \in \mathbb{R}^K$ for each data point $\mathbf{x}_n$
with lower dimension, $K < D$. The set of principal axes
$\mathbf{W}$ relates our latent variables to our actual data.

Specifically, we have that
\begin{equation*}
\mathbf{z} \sim N(0, \mathbf{I}),
\end{equation*}
and that
\begin{equation*}
\mathbf{x} \mid \mathbf{z} \sim N(\mathbf{Wz} + \mu, \sigma^2\mathbf{I}),
\end{equation*}
where $\mathbf{\epsilon} \sim N(0, \sigma^2\mathbf{I})$.

Because of the dependence on $\mathbf{Wz}$, we don't want to solely
want to infer the original principal axes (as would be done in the
vanilla PCA model). Rather, we aim to model the principal axes with
respect to the latent variables.

For our marginal distribution, we get that
\begin{equation*}
x \sim N(0, \mathbf{W}\mathbf{W}^Y + \sigma^2\mathbf{I}).
\end{equation*}

Note here that regular PCA is simply the specific case of
Probabilistic PCA, as $\sigma^2 \to 0$.

We set up our model below, forming creating a prior distribution over
our variables of interest.

\begin{lstlisting}[language=Python]
from edward.models import Normal

w = Normal(mu=tf.zeros([D, K]), sigma=2.0 * tf.ones([D, K]))
z = Normal(mu=tf.zeros([N, K]), sigma=tf.ones([N, K]))
x = Normal(mu=tf.matmul(w, z, transpose_b=True), sigma=tf.ones([D, N]))

\end{lstlisting}

\subsubsection{Inference}

Since the principal axes $\mathbf{W}$ cannot be analytically
determined, we must use some inference method. Below, we set up our
inference variables and then run a chosen algorithm. For this
example, we minimize the $\text{KL}(q\|p)$ divergence measure.

\begin{lstlisting}[language=Python]
qw = Normal(mu=tf.Variable(tf.random_normal([D, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([D, K]))))
qz = Normal(mu=tf.Variable(tf.random_normal([N, K])),
            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([N, K]))))

inference = ed.KLqp({w: qw, z: qz}, data={x: x_train})
inference.run(n_iter=500, n_print=100, n_samples=10)
\end{lstlisting}

\subsubsection{Criticism}

We first inspect the model's inferred principal axes.

\begin{lstlisting}[language=Python]
sess = ed.get_session()
print("Inferred principal axes:")
print(sess.run(qw.mean()))
\end{lstlisting}

\begin{lstlisting}
## Inferred principal axes:
## [[-0.24093632]
##  [-1.76468039]]
\end{lstlisting}

The model has recovered the true principal axes up to finite data and
also up to identifiability (there's a symmetry in the
parameterization).

Another way to criticize the model is to visualize the observed data
against data generated from our fitted model.

\begin{lstlisting}[language=Python]
x_post = ed.copy(x, {w: qw, z: qz})
x_gen = sess.run(x_post)

plt.scatter(x_gen[0, :], x_gen[1, :], color='red', alpha=0.1)
plt.axis([-10, 10, -10, 10])
plt.title("Data generated from model")
plt.show()
\end{lstlisting}

\includegraphics[width=450px]{/images/probabilistic-pca-fig1.png}

The generated data looks close to the true data.

\subsubsection{Acknowledgments}

We thank Mayank Agrawal for writing the initial version of this
tutorial.

\subsubsection{References}\label{references}
