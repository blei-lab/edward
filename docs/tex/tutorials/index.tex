\title{Tutorials}

\subsection{Tutorials}

Edward provides a testbed for rapid experimentation and research with
probabilistic models. Here we show how to apply this process for
complicated learning tasks.

% \href{#}{Conjugate models}

\href{supervised-regression}{Bayesian linear regression} \\
A fundamental model for supervised learning.

% \href{#}{Hierarchical linear models} \\
% \href{hierarchical-logistic}{Hierarchical logistic regression}
% Sharing statistical strength across parallel experiments.

\href{supervised-classification}{Gaussian process classification} \\
Learning a distribution over functions for supervised classification.

\href{unsupervised}{Mixture models} \\
Unsupervised learning by clustering data points.

% \href{#}{Latent Dirichlet allocation} \\
% Discovering topics in 1.7 million New York Times articles.

% \href{#}{Cox process} \\
% A spatial analysis of professional basketball players.

% \href{#}{Stochastic variational inference} \\
% With conjugate inferences.

\href{bbvi}{Black box variational inference} [todo] \\
...

\href{latent-space-models}{Latent space models} \\
Analyzing connectivity patterns in neural data.

\href{mixture-density-network}{Mixture density networks} \\
A neural density model for solving inverse problems.

% \href{vae}{Variational auto-encoder} \\
% Building a deep generative model of MNIST digits.

% \href{#}{Hamiltonian Monte Carlo} \\
% Incorporating geometry of the parameter space for efficient exploration.

% \href{#}{Rejection sampling variational inference} \\
% Using reparameterization to take efficient gradients of
% difficult-to-sample distributions.

\href{gan}{Generative adversarial networks} \\
Building a deep generative model of MNIST digits.

\href{decoder}{Probabilistic decoder} [todo] \\
A model of latent codes in information theory.

\href{inference-networks}{Inference networks} \\
How to amortize computation for training and testing models.

\href{mmd}{Maximum mean discrepancy} [todo] \\
A kernelized test statistic for comparing two distributions.

% \href{#}{Posterior dispersion indices} \\
% Checking posterior uncertainty at a per-datapoint level.

% \href{bayesian-neural-network}{Bayesian neural network} [todo] \\
% Bayesian analysis with neural networks.

If you're interested in contributing a tutorial, checking out the
\href{/contributing}{contributing page}.
For more background and notation, see the pages below.
\begin{itemize}
  \item \href{model}{Probabilistic models}
  \item \href{inference}{Inference of probabilistic models}
  \begin{itemize}
   \item \href{variational-inference}{Variational inference}
   \item \href{klqp}{$\text{KL}(q\|p)$ minimization}
   \item \href{klpq}{$\text{KL}(p\|q)$ minimization}
   \item \href{map}{Maximum a posteriori estimation}
   \item \href{map-laplace}{Laplace approximation}
  \end{itemize}
  \item \href{criticism}{Model criticism}
\end{itemize}
