\title{Tutorials}

\subsection{Tutorials}

Edward provides a testbed for rapid experimentation and research with
probabilistic models. Here we show how this process is applied to
capture complex machine learning tasks.

%\href{conjugate}{Conjugate models}

% \href{supervised-regression}{Bayesian linear regression} \\
% A fundamental model for supervised learning (regression).

\href{#}{Hierarchical linear models} \\
Sharing statistical strength across parallel experiments.

%\href{hierarchical-logistic}{Hierarchical logistic regression}

% \href{supervised-classification}{Gaussian process classification} \\
% Learning a distribution over functions for supervised learning (classification).

% \href{unsupervised}{Mixture models} \\
% Unsupervised learning with clusters.

\href{unsupervised}{Latent Dirichlet allocation} \\
Discovering topics in 1.7 million New York Times articles.

\href{#}{Cox process} \\
A spatial analysis of professional basketball players.

% \href{latent-space-models}{Latent space models} \\
\href{#}{Exponential family embeddings} \\
Analyzing connectivity patterns in neural data with embedding models.

% \href{mixture-density-network}{Mixture density networks} \\
% A supervised model for density estimation, leveraging neural networks.

% \href{vae}{Variational auto-encoder} [todo] \\
% Building a deep generative model of MNIST digits.

% \href{#}{Variational models} \\
% Using probabilistic models to design rich variational approximations.

% \href{#}{Operator variational inference} \\
% How to trade computational and statistical tradeoffs in variational
% inference.

\href{#}{Hamiltonian Monte Carlo} \\
Incorporating geometry of the parameter space for efficient exploration.

\href{#}{Rejection sampling variational inference} \\
Using reparameterization to take efficient gradients for
difficult-to-sample distributions.

\href{gan}{Generative adversarial networks} \\
Building a deep generative model of MNIST digits.

% \href{decoder}{Probabilistic decoder} [todo] \\
\href{#}{Probabilistic decoder} \\
A model of latent codes in information theory.

% \href{inference-networks}{Inference networks} [todo] \\
\href{#}{Inference networks} \\
How to amortize computation for training and testing models.

\href{#}{Maximum mean discrepancy} \\
A kernelized test statistic for comparing two distributions.

\href{#}{Posterior dispersion indices} \\
Analyzing posterior uncertainty at a per-datapoint level.

% \href{bayesian-neural-network}{Bayesian neural network} [todo] \\
% Bayesian analysis with neural networks.

% For more background and notation, see the pages below.
% \begin{itemize}
%   \item \href{model}{Probabilistic models}
%   \item \href{inference}{Bayesian inference}
%   \begin{itemize}
%    \item \href{variational-inference}{Variational inference}
%    \item \href{klqp}{$\text{KL}(q\|p)$ minimization}
%    \item \href{klpq}{$\text{KL}(p\|q)$ minimization}
%    \item \href{map}{Maximum a posteriori estimation}
%    \begin{itemize}
%      \item \href{map-laplace}{Laplace approximation}
%    \end{itemize}
%    %\item \href{iwvi}{Importance-weighted variational inference}
%    %\item \href{#}{Variational expectation maximization}
%   \end{itemize}
%   \item \href{criticism}{Model criticism}
% \end{itemize}

% Here we show how this process is applied to solve real-world applications.
% \begin{itemize}
%   \item \href{c-elegans}{Analyzing connectivity patterns in neural data}
%   \item \href{new-york-times}{Discovering topics in New York Times articles}
%   \item \href{genome}{Ancestral populations from human genomes}
%   \item \href{taxi}{Analyzing taxi rides from the city of Porto}
%   \item \href{basketball}{A spatial analysis of professional basketball}
%   \item \href{mnist}{Building a generative model of MNIST digits}
%   \item \href{text8}{Building a language model for text from Wikipedia}
%   \item \href{eight-schools}{Meta-analysis of parallel experiments in 8 schools}
% \end{itemize}
